#!/usr/bin/env python3
"""
FACE RECOGNITION SYSTEM - INSIGHTFACE + DEEPFACE + YOLO-POSE + ATTENDANCE + REAL-TIME BACKEND
GPU/CPU DUAL MODE - AUTO FALLBACK TO CPU
WITH FLASK API FOR WEB CONTROL
WITH ENGAGEMENT SCORE CALCULATION BASED ON EMOTION AND BEHAVIOR
"""

import os
# Fix numpy version issue ƒë·∫ßu ti√™n
import random
import sys

# Fix cho numpy version m·ªõi
try:
    import numpy
    if hasattr(numpy, '_core'):
        numpy.core.multiarray = numpy._core.multiarray
except:
    pass

import json
import logging
import pickle
import subprocess
import threading
import time
import warnings
from collections import Counter, defaultdict, deque

from flask import Response  # üî¥ TH√äM import n√†y

warnings.filterwarnings('ignore', category=FutureWarning)
import time
from datetime import datetime

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import requests
import torch
# ==================== FLASK API ====================
from flask import Flask, jsonify, request
from flask_cors import CORS

# ==================== MOVE SKLEARN IMPORTS L√äN SAU ====================
# Import sklearn SAU KHI ƒë√£ fix numpy
try:
    from sklearn.metrics import accuracy_score
    from sklearn.preprocessing import Normalizer
    from sklearn.svm import SVC
    SKLEARN_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Warning: scikit-learn not available: {e}")
    SKLEARN_AVAILABLE = False
    # Define placeholders
    class Normalizer:
        def __init__(self, norm='l2'):
            self.norm = norm
        def transform(self, X):
            return X / np.linalg.norm(X, axis=1, keepdims=True)
    
    class SVC:
        def __init__(self, **kwargs):
            print("‚ö†Ô∏è SVC is a placeholder - install scikit-learn")
        def fit(self, X, y):
            pass
        def predict(self, X):
            return ['Unknown'] * len(X)
        def predict_proba(self, X):
            return np.zeros((len(X), 1))
        @property
        def classes_(self):
            return np.array(['Unknown'])

# ==================== TH√äM IMPORT CHO DEEPFACE ====================
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    print("‚úÖ DeepFace ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng")
except ImportError as e:
    print(f"‚ö†Ô∏è DeepFace not available: {e}")
    DEEPFACE_AVAILABLE = False

# Kh·ªüi t·∫°o Flask app
app = Flask(__name__)
CORS(app)

# Bi·∫øn to√†n c·ª•c cho AI system
ai_running = False
ai_thread = None
system = None
last_detection_results = []
last_detection_time = None
ai_status_lock = threading.Lock()
detection_lock = threading.Lock()  # üî¥ TH√äM: Lock cho detection results

# ==================== TH√äM IMPORT CHO YAML ====================
try:
    import yaml
except ImportError:
    print("üì• Installing pyyaml for YOLO...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "pyyaml"])
    import yaml
    
# ==================== ENGAGEMENT CALCULATOR ====================
import numpy as np


class CameraManager:
    """Qu·∫£n l√Ω camera d√πng chung cho AI v√† streaming"""
    
    def __init__(self, camera_index=0):
        self.camera_index = camera_index
        self.cap = None
        self.frame = None
        self.lock = threading.Lock()
        self.is_running = False
        self.last_read_time = 0
        self.read_errors = 0
        self.max_errors = 10
        
    def start(self):
        """Kh·ªüi ƒë·ªông camera v·ªõi retry mechanism"""
        if self.cap is not None and self.cap.isOpened():
            self.cap.release()
            time.sleep(0.5)
        
        print(f"üîç ƒêang k·∫øt n·ªëi camera index {self.camera_index}...")
        
        # Th·ª≠ c√°c camera index kh√°c nhau
        camera_indices = [self.camera_index]
        if self.camera_index == 0:
            camera_indices = [0, 1, 2, 3, 4]
        elif self.camera_index == 1:
            camera_indices = [1, 0, 2, 3, 4]
        else:
            camera_indices = [self.camera_index, 0, 1, 2, 3]
        
        for idx in camera_indices:
            try:
                print(f"  Th·ª≠ camera index {idx}...")
                
                # D√πng CAP_DSHOW cho Windows, CAP_V4L2 cho Linux
                if sys.platform == 'win32':
                    self.cap = cv2.VideoCapture(idx, cv2.CAP_DSHOW)
                else:
                    self.cap = cv2.VideoCapture(idx, cv2.CAP_V4L2)
                
                if self.cap.isOpened():
                    # Test ƒë·ªçc frame
                    ret, test_frame = self.cap.read()
                    if ret and test_frame is not None:
                        self.camera_index = idx
                        # C·∫•u h√¨nh camera
                        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                        self.cap.set(cv2.CAP_PROP_FPS, 30)
                        
                        # Ki·ªÉm tra th·ª±c t·∫ø
                        actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        actual_fps = int(self.cap.get(cv2.CAP_PROP_FPS))
                        
                        print(f"‚úÖ Connected to camera index {idx}")
                        print(f"   Resolution: {actual_width}x{actual_height}")
                        print(f"   FPS: {actual_fps}")
                        
                        self.is_running = True
                        self.read_errors = 0
                        return True
                    else:
                        print(f"  ‚ùå Camera {idx}: Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c test frame")
                        self.cap.release()
                        self.cap = None
                else:
                    print(f"  ‚ùå Camera {idx}: Kh√¥ng m·ªü ƒë∆∞·ª£c")
                    
            except Exception as e:
                print(f"  ‚ùå Camera {idx} error: {str(e)}")
                if self.cap is not None:
                    self.cap.release()
                    self.cap = None
        
        print(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn b·∫•t k·ª≥ camera n√†o (ƒë√£ th·ª≠ {camera_indices})")
        return False
    
    def read_frame(self):
        """ƒê·ªçc frame t·ª´ camera v·ªõi error handling"""
        if self.cap is None or not self.cap.isOpened():
            print("‚ö†Ô∏è Camera ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông, ƒëang th·ª≠ kh·ªüi ƒë·ªông l·∫°i...")
            if self.start():
                time.sleep(0.5)
            else:
                return None
        
        try:
            ret, frame = self.cap.read()
            
            if not ret or frame is None:
                self.read_errors += 1
                print(f"‚ö†Ô∏è L·ªói ƒë·ªçc frame #{self.read_errors}")
                
                if self.read_errors >= self.max_errors:
                    print("üîÑ Qu√° nhi·ªÅu l·ªói, ƒëang kh·ªüi ƒë·ªông l·∫°i camera...")
                    self.stop()
                    time.sleep(1)
                    if self.start():
                        self.read_errors = 0
                        time.sleep(0.5)
                        # Th·ª≠ ƒë·ªçc l·∫°i
                        ret, frame = self.cap.read()
                        if ret and frame is not None:
                            with self.lock:
                                self.frame = frame.copy()
                            return frame
                
                return None
            
            # Reset error counter
            self.read_errors = 0
            
            # L∆∞u frame m·ªõi nh·∫•t
            with self.lock:
                self.frame = frame.copy()
                self.last_read_time = time.time()
            
            return frame
            
        except Exception as e:
            print(f"‚ùå Exception khi ƒë·ªçc frame: {str(e)}")
            self.read_errors += 1
            return None
    
    def get_latest_frame(self):
        """L·∫•y frame m·ªõi nh·∫•t cho streaming"""
        with self.lock:
            if self.frame is not None:
                return self.frame.copy()
        
        # N·∫øu kh√¥ng c√≥ frame, th·ª≠ ƒë·ªçc ngay l·∫≠p t·ª©c
        return self.read_frame()
    
    def stop(self):
        """D·ª´ng camera"""
        if self.cap is not None:
            self.cap.release()
            self.cap = None
        self.is_running = False
        self.frame = None
        print("‚úÖ Camera stopped")

# üî¥ TH√äM: Global camera manager
camera_manager = CameraManager(camera_index=0)

class EngagementCalculator:
    """L·ªõp t√≠nh to√°n ƒë·ªô t·∫≠p trung d·ª±a tr√™n c·∫£m x√∫c v√† h√†nh vi"""
    
    def __init__(self):
        # Tr·ªçng s·ªë c·∫£m x√∫c (0-100)
        self.emotion_weights = {
            'neutral': 70,    # B√¨nh th∆∞·ªùng
            'happy': 85,      # Vui v·∫ª - t√≠ch c·ª±c
            'surprised': 65,  # Ng·∫°c nhi√™n
            'surprise': 65,   # Alias
            'sad': 40,        # Bu·ªìn
            'sadness': 40,    # Alias
            'angry': 30,      # T·ª©c gi·∫≠n
            'anger': 30,      # Alias
            'fear': 35,       # S·ª£ h√£i
            'disgust': 40,    # Kh√≥ ch·ªãu
            'fearful': 35,    # Alias
            'disgusted': 40   # Alias
        }
        
        # Tr·ªçng s·ªë h√†nh vi (0-100)
        self.behavior_weights = {
            'writing': 90,              # ƒêang vi·∫øt - r·∫•t t·∫≠p trung
            'look_straight': 80,        # Nh√¨n th·∫≥ng - t·∫≠p trung
            'raising_one_hand': 75,     # Gi∆° m·ªôt tay - tham gia
            'raising_two_hands': 78,    # Gi∆° hai tay - t√≠ch c·ª±c
            'raising_hand': 75,         # Alias
            'normal': 60,               # B√¨nh th∆∞·ªùng
            'look_around': 35,          # Nh√¨n quanh - ph√¢n t√¢m
            'distracted': 30,           # M·∫•t t·∫≠p trung
            'unknown': 50,              # Kh√¥ng x√°c ƒë·ªãnh
            '': 50                      # Empty behavior
        }
        
        # H·ªá s·ªë ƒëi·ªÅu ch·ªânh confidence
        self.confidence_factors = {
            'high': 1.05,    # confidence > 0.8
            'medium': 1.0,   # confidence 0.5-0.8
            'low': 0.95      # confidence < 0.5
        }
        
        # History ƒë·ªÉ smoothing
        self.engagement_history = {}
        self.history_length = 5
    
    def _normalize_emotion_behavior_scores(self, emotion_score, behavior_score):
        """Chu·∫©n h√≥a ƒëi·ªÉm c·∫£m x√∫c v√† h√†nh vi v·ªÅ kho·∫£ng 0-100"""
        # Gi·ªõi h·∫°n t·ª´ 0-100
        emotion_score = max(0, min(100, emotion_score))
        behavior_score = max(0, min(100, behavior_score))
        
        return emotion_score, behavior_score
    
    def get_confidence_factor(self, confidence):
        """X√°c ƒë·ªãnh h·ªá s·ªë d·ª±a tr√™n confidence"""
        if confidence >= 0.8:
            return self.confidence_factors['high']
        elif confidence >= 0.5:
            return self.confidence_factors['medium']
        else:
            return self.confidence_factors['low']
    
    def calculate_engagement(self, student_id, emotion, emotion_confidence, 
                            behavior, behavior_confidence=None, bbox=None):
        """
        T√≠nh ƒëi·ªÉm t·∫≠p trung (0-100)
        
        Args:
            student_id: ID h·ªçc sinh
            emotion: C·∫£m x√∫c (string)
            emotion_confidence: ƒê·ªô tin c·∫≠y c·∫£m x√∫c (0-1)
            behavior: H√†nh vi (string)
            behavior_confidence: ƒê·ªô tin c·∫≠y h√†nh vi (0-1)
            bbox: Bounding box (optional, cho spatial analysis)
        """
        
        # 1. Chu·∫©n h√≥a ƒë·∫ßu v√†o
        emotion = emotion.lower() if emotion else 'neutral'
        behavior = behavior.lower() if behavior else 'normal'
        
        # M·∫∑c ƒë·ªãnh confidence n·∫øu kh√¥ng c√≥
        if behavior_confidence is None:
            behavior_confidence = 0.7  # Gi·∫£ ƒë·ªãnh moderate confidence
        
        # 2. L·∫•y tr·ªçng s·ªë c∆° b·∫£n
        emotion_weight = self.emotion_weights.get(emotion, 50)
        behavior_weight = self.behavior_weights.get(behavior, 50)
        
        # 3. T√≠nh to√°n ƒëi·ªÉm c∆° b·∫£n v·ªõi confidence
        emotion_score = emotion_weight * emotion_confidence
        behavior_score = behavior_weight * behavior_confidence
        
        # 4. Chu·∫©n h√≥a v·ªÅ kho·∫£ng 0-100
        emotion_score, behavior_score = self._normalize_emotion_behavior_scores(
            emotion_score, behavior_score
        )
        
        # 5. K·∫øt h·ª£p ƒëi·ªÉm (40% c·∫£m x√∫c, 60% h√†nh vi)
        base_engagement = (emotion_score * 0.4 + behavior_score * 0.6)
        
        # 6. √Åp d·ª•ng h·ªá s·ªë confidence
        emotion_conf_factor = self.get_confidence_factor(emotion_confidence)
        behavior_conf_factor = self.get_confidence_factor(behavior_confidence)
        confidence_factor = (emotion_conf_factor + behavior_conf_factor) / 2
        
        # 7. H·ªá s·ªë ƒë·∫∑c bi·ªát
        special_factors = self._calculate_special_factors(behavior, bbox)
        
        # 8. T√≠nh to√°n cu·ªëi c√πng
        adjusted_engagement = base_engagement * confidence_factor * special_factors
        
        # 9. GI·ªöI H·∫†N NGHI√äM NG·∫∂T trong kho·∫£ng 0-100
        final_engagement = max(0, min(100, adjusted_engagement))
        
        # 10. L√†m m∆∞·ª£t v·ªõi history
        smoothed_engagement = self._apply_smoothing(student_id, final_engagement)
        
        # 11. ƒê·∫£m b·∫£o cu·ªëi c√πng v·∫´n n·∫±m trong 0-100
        smoothed_engagement = max(0, min(100, smoothed_engagement))
        
        # 12. Ph√¢n lo·∫°i m·ª©c ƒë·ªô t·∫≠p trung
        concentration_level = self._classify_concentration(smoothed_engagement)
        
        return {
            'engagement_score': round(smoothed_engagement, 2),
            'concentration_level': concentration_level,
            'base_components': {
                'emotion': {
                    'type': emotion,
                    'weight': emotion_weight,
                    'confidence': emotion_confidence,
                    'score': round(emotion_score, 2)
                },
                'behavior': {
                    'type': behavior,
                    'weight': behavior_weight,
                    'confidence': behavior_confidence,
                    'score': round(behavior_score, 2)
                }
            },
            'adjustments': {
                'confidence_factor': round(confidence_factor, 3),
                'special_factors': round(special_factors, 3),
                'base_engagement': round(base_engagement, 2),
                'final_engagement': round(final_engagement, 2)
            }
        }
    
    def _calculate_special_factors(self, behavior, bbox):
        """T√≠nh h·ªá s·ªë ƒë·∫∑c bi·ªát d·ª±a tr√™n h√†nh vi v√† v·ªã tr√≠"""
        factor = 1.0
        
        # Boost cho h√†nh vi t√≠ch c·ª±c
        if 'writing' in behavior:
            factor *= 1.05
        elif 'raising' in behavior:
            factor *= 1.03
        elif 'look_straight' in behavior:
            factor *= 1.02
        
        # Penalty cho h√†nh vi ti√™u c·ª±c
        if 'look_around' in behavior or 'distracted' in behavior:
            factor *= 0.90
        
        # N·∫øu c√≥ bbox, th√™m spatial analysis
        if bbox:
            try:
                x, y, w, h = bbox
                center_x = x + w/2
                center_y = y + h/2
                
                # Gi·∫£ ƒë·ªãnh frame width=640, height=480
                frame_center_x = 320
                frame_center_y = 240
                
                # Kho·∫£ng c√°ch ƒë·∫øn center
                distance = np.sqrt((center_x - frame_center_x)**2 + (center_y - frame_center_y)**2)
                
                # H·ªá s·ªë d·ª±a tr√™n v·ªã tr√≠
                if distance < 100:
                    factor *= 1.02
                elif distance > 300:
                    factor *= 0.98
            except:
                pass
        
        return min(1.1, max(0.9, factor))  # Gi·ªõi h·∫°n h·ªá s·ªë ƒë·∫∑c bi·ªát
    
    def _apply_smoothing(self, student_id, current_score):
        """√Åp d·ª•ng moving average ƒë·ªÉ l√†m m∆∞·ª£t k·∫øt qu·∫£"""
        if student_id not in self.engagement_history:
            self.engagement_history[student_id] = []
        
        history = self.engagement_history[student_id]
        history.append(current_score)
        
        # Gi·ªØ l·ªãch s·ª≠ t·ªëi ƒëa
        if len(history) > self.history_length:
            history.pop(0)
        
        # Weighted moving average (m·ªõi h∆°n -> n·∫∑ng h∆°n)
        if len(history) > 0:
            weights = np.linspace(0.5, 1.0, len(history))
            weights = weights / weights.sum()
            smoothed = np.average(history, weights=weights)
            return float(smoothed)
        
        return current_score
    
    def _classify_concentration(self, score):
        """Ph√¢n lo·∫°i m·ª©c ƒë·ªô t·∫≠p trung"""
        if score >= 80:
            return "very_high"
        elif score >= 70:
            return "high"
        elif score >= 60:
            return "medium"
        elif score >= 50:
            return "low"
        else:
            return "very_low"
    
    def get_engagement_report(self, student_data_list):
        """T·∫°o b√°o c√°o t·∫≠p trung cho t·∫•t c·∫£ h·ªçc sinh"""
        report = {
            'total_students': len(student_data_list),
            'average_engagement': 0,
            'concentration_distribution': {
                'very_high': 0, 'high': 0, 'medium': 0, 
                'low': 0, 'very_low': 0
            },
            'students': []
        }
        
        total_score = 0
        
        for student in student_data_list:
            engagement_result = self.calculate_engagement(
                student_id=student.get('id'),
                emotion=student.get('emotion', 'neutral'),
                emotion_confidence=student.get('emotion_confidence', 0.5),
                behavior=student.get('behavior', 'normal'),
                behavior_confidence=student.get('behavior_confidence', 0.7),
                bbox=student.get('bbox')
            )
            
            # Th√™m v√†o b√°o c√°o
            report['students'].append({
                'id': student.get('id'),
                'name': student.get('name', 'Unknown'),
                'engagement': engagement_result['engagement_score'],
                'concentration_level': engagement_result['concentration_level'],
                'emotion': student.get('emotion'),
                'behavior': student.get('behavior')
            })
            
            # C·∫≠p nh·∫≠t th·ªëng k√™
            total_score += engagement_result['engagement_score']
            report['concentration_distribution'][engagement_result['concentration_level']] += 1
        
        if report['total_students'] > 0:
            report['average_engagement'] = round(total_score / report['total_students'], 2)
        
        return report
    
    def _get_engagement_color(self, score):
        """L·∫•y m√†u d·ª±a tr√™n engagement score"""
        if score >= 80:
            return (0, 255, 0)  # Xanh l√° - r·∫•t t·ªët
        elif score >= 70:
            return (0, 200, 0)  # Xanh l√° nh·∫°t - t·ªët
        elif score >= 60:
            return (255, 255, 0)  # V√†ng - trung b√¨nh
        elif score >= 50:
            return (255, 165, 0)  # Cam - th·∫•p
        else:
            return (255, 0, 0)  # ƒê·ªè - r·∫•t th·∫•p

# ==================== BACKEND DATA SENDER - ENHANCED ====================
class EnhancedBackendDataSender:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.is_connected = False
        self.last_attendance_sent = {}
        self.last_behavior_sent = {}
        self.last_emotion_sent = {}
        self.session_id = f"session_{int(time.time())}"
        
        # üî¥ TH√äM: Mapping t√™n sang ID c·ªë ƒë·ªãnh
        self.student_name_to_id = {
            # H·ªçc sinh t·ª´ d·ªØ li·ªáu m·∫´u
            "Dino": "SV001",
            "Thinh": "SV003",
            "Minh": "SV002",
            "Mini": "SV004",
            "Khoa": "SV005",
            "Nam": "SV006",
            "Thanh": "SV007",
        }
        
        # üî¥ TH√äM: Reverse mapping (ID -> T√™n)
        self.student_id_to_name = {
            "SV001": "Dino",
            "SV002": "Minh",
            "SV003": "Thinh",
            "SV004": "Mini",
            "SV005": "Khoa",
            "SV006": "Nam",
            "SV007": "Thanh",
        }
        
        # üî¥ TH√äM: Lock cho pending requests
        self.request_lock = threading.Lock()
        self.pending_requests = {}
        
        # üî¥ TH√äM: Danh s√°ch t·ª´ kh√≥a "unknown" ƒë·ªÉ l·ªçc
        self.unknown_keywords = ['unknown', 'unknow', 'kh√¥ng r√µ', 'ch∆∞a bi·∫øt', 'unknown student', 'unidentified']
        
        self.test_connection()
        self.setup_headers()
        
        # üî¥ TH√äM: Batch endpoint
        self.batch_endpoint = f"{base_url}/api/ai/batch-process"
        print(f"‚úÖ Backend sender initialized with FIXED ID mapping.")
        print(f"üì¶ Batch endpoint: {self.batch_endpoint}")
        print(f"üìä Student mapping: {len(self.student_name_to_id)} names mapped")

        self._test_connection_safe()
        
        # üî¥ TH√äM: Queue cho async processing
        self.request_queue = []
        self.queue_lock = threading.Lock()
        self.max_queue_size = 50
        
        # üî¥ TH√äM: Background thread x·ª≠ l√Ω queue
        self.processor_thread = threading.Thread(target=self._process_queue, daemon=True)
        self.processor_thread.start()
        
        print(f"‚úÖ Backend sender initialized with async processing and fixed ID system.")
    
    def setup_headers(self):
        """Thi·∫øt l·∫≠p headers cho requests"""
        self.headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'AI-Recognition-System/1.0'
        }

    def _test_connection_safe(self):
        """Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn backend an to√†n"""
        try:
            response = requests.get(f"{self.base_url}/api/health", timeout=2)
            if response.status_code == 200:
                self.is_connected = True
                print("‚úÖ ƒê√£ k·∫øt n·ªëi ƒë·∫øn backend th√†nh c√¥ng!")
                return True
            else:
                print(f"‚ö†Ô∏è Backend tr·∫£ v·ªÅ m√£ l·ªói: {response.status_code}")
                self.is_connected = False
                return False
        except requests.exceptions.ConnectionError:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn backend. Ch·∫°y ·ªü ch·∫ø ƒë·ªô offline.")
            self.is_connected = False
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ki·ªÉm tra k·∫øt n·ªëi: {str(e)}")
            self.is_connected = False
            return False
    
    def test_connection(self):
        """Alias cho _test_connection_safe"""
        return self._test_connection_safe()
    
    # ==================== CORE: H√ÄM CHUY·ªÇN ƒê·ªîI ID ====================
    def get_fixed_student_id(self, student_name, raw_student_id=None):
        """
        Chuy·ªÉn ƒë·ªïi t√™n h·ªçc sinh sang ID c·ªë ƒë·ªãnh
        
        Args:
            student_name: T√™n h·ªçc sinh (string)
            raw_student_id: ID th√¥ t·ª´ AI (optional)
            
        Returns:
            ID c·ªë ƒë·ªãnh (string) ho·∫∑c None n·∫øu l√† unknown
        """
        if not student_name or student_name.strip() == "":
            # N·∫øu kh√¥ng c√≥ t√™n, tr·∫£ v·ªÅ None (kh√¥ng g·ª≠i)
            return None
        
        # Chu·∫©n h√≥a t√™n
        name_lower = student_name.lower().strip()
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i "unknown" kh√¥ng
        for keyword in self.unknown_keywords:
            if keyword in name_lower:
                return None  # Tr·∫£ v·ªÅ None ƒë·ªÉ kh√¥ng g·ª≠i
        
        # T√¨m trong mapping (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        for mapped_name, mapped_id in self.student_name_to_id.items():
            if mapped_name.lower() == name_lower:
                return mapped_id
        
        # T√¨m partial match
        for mapped_name, mapped_id in self.student_name_to_id.items():
            if mapped_name.lower() in name_lower or name_lower in mapped_name.lower():
                print(f"üîç Partial match: '{student_name}' -> '{mapped_name}' ({mapped_id})")
                return mapped_id
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o ID m·ªõi d·ª±a tr√™n hash c·ªßa t√™n
        # ƒê·∫£m b·∫£o lu√¥n tr·∫£ v·ªÅ ID d·∫°ng SVxxx
        name_hash = abs(hash(student_name)) % 1000
        new_id = f"SV{name_hash + 100:03d}"  # SV100 ƒë·∫øn SV999
        
        # Th√™m v√†o mapping ƒë·ªÉ d√πng sau
        self.student_name_to_id[student_name] = new_id
        self.student_id_to_name[new_id] = student_name
        
        print(f"üìù Created new mapping: '{student_name}' -> {new_id}")
        return new_id
    
    def get_student_name_from_id(self, student_id):
        """L·∫•y t√™n h·ªçc sinh t·ª´ ID"""
        return self.student_id_to_name.get(student_id, "Unknown Student")
    
    def add_student_mapping(self, student_name, student_id):
        """Th√™m mapping m·ªõi"""
        if student_name and student_id:
            self.student_name_to_id[student_name] = student_id
            self.student_id_to_name[student_id] = student_name
            print(f"‚ûï Added mapping: '{student_name}' <-> {student_id}")
    
    def _process_queue(self):
        """Background thread x·ª≠ l√Ω queue"""
        while True:
            time.sleep(0.1)  # Ki·ªÉm tra queue m·ªói 100ms
            
            with self.queue_lock:
                if not self.request_queue:
                    continue
                
                # L·∫•y batch ƒë·ªÉ x·ª≠ l√Ω (t·ªëi ƒëa 5 request c√πng l√∫c)
                batch = self.request_queue[:10]
                self.request_queue = self.request_queue[10:]
            
            # X·ª≠ l√Ω batch trong thread ri√™ng
            if batch:
                thread = threading.Thread(target=self._process_batch, args=(batch,), daemon=True)
                thread.start()
    
    def _process_batch(self, batch):
        """X·ª≠ l√Ω batch requests"""
        threads = []
        for request_data in batch:
            thread = threading.Thread(
                target=self._send_request_async,
                args=(request_data['endpoint'], request_data['data'], request_data['request_type']),
                daemon=True
            )
            thread.start()
            threads.append(thread)
    
    def _send_request_async(self, endpoint, data, request_type):
        """G·ª≠i request async (kh√¥ng blocking)"""
        try:
            # üî¥ TH√äM: Ki·ªÉm tra n·∫øu t√™n h·ªçc sinh l√† "Unknown"
            student_name = data.get('student_name', data.get('name', 'Unknown'))
            if self._is_unknown_name(student_name):
                return  # Kh√¥ng g·ª≠i n·∫øu l√† unknown
            
            response = requests.post(
                endpoint,
                json=data,
                headers=self.headers,
                timeout=3
            )
            
            if response.status_code == 200:
                # Log ng·∫Øn g·ªçn
                student_id = data.get('student_id', 'N/A')
                print(f"üì§ {request_type[:3]}: {student_name[:10]} ({student_id})")
            else:
                # Kh√¥ng log error ƒë·ªÉ tr√°nh spam
                pass
                
        except Exception as e:
            # B·ªè qua l·ªói network
            pass
    
    def _is_unknown_name(self, name):
        """Ki·ªÉm tra xem t√™n c√≥ ph·∫£i l√† unknown kh√¥ng"""
        if not name:
            return True
        
        name_lower = str(name).lower().strip()
        
        # Ki·ªÉm tra c√°c t·ª´ kh√≥a unknown
        for keyword in self.unknown_keywords:
            if keyword in name_lower:
                return True
        
        # Ki·ªÉm tra n·∫øu t√™n qu√° ng·∫Øn ho·∫∑c ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát
        if len(name_lower) < 2:
            return True
        
        return False
    
    def _get_concentration_level(self, engagement):
        """Map engagement score to concentration level"""
        if engagement >= 0.8:
            return "high"
        elif engagement >= 0.6:
            return "medium"
        else:
            return "low"
    
    # üî¥ TH√äM: Simple batch sender v·ªõi ID c·ªë ƒë·ªãnh
    def send_detection_batch(self, detections, fps, frame_count):
        """G·ª≠i detection batch ƒë∆°n gi·∫£n (non-queue) v·ªõi ID c·ªë ƒë·ªãnh - KH√îNG G·ª¨I UNKNOWN"""
        if not self.is_connected or not detections:
            return False

        try:
            batch_data = {
                "type": "detection_update",
                "session_id": f"session_{int(time.time())}",
                "timestamp": datetime.now().isoformat(),
                "fps": fps,
                "frame_count": frame_count,
                "data": []
            }
    
            for face in detections:
                behavior_text = face.get("behavior")
                engagement = face.get("engagement")
                focus_score = float(engagement)
                student_name = face.get("name", "Unknown")
                
                # üî¥ S·ª¨A: Ki·ªÉm tra n·∫øu l√† unknown th√¨ b·ªè qua
                if self._is_unknown_name(student_name):
                    continue  # B·ªè qua kh√¥ng g·ª≠i
                
                # üî¥ S·ª¨A: D√πng ID c·ªë ƒë·ªãnh
                raw_student_id = face.get("id", f"ID_{hash(str(face)) % 10000:04d}")
                fixed_student_id = self.get_fixed_student_id(student_name, raw_student_id)
                
                # üî¥ TH√äM: Ki·ªÉm tra n·∫øu fixed_student_id l√† None (unknown) th√¨ b·ªè qua
                if fixed_student_id is None:
                    continue
        
                now = datetime.now()
        
                item = {
                    # üî¥ D√ôNG ID C·ªê ƒê·ªäNH
                    "student_id": fixed_student_id,
                    "student_name": student_name,
                
                    # üî¥ C√ÅC TR∆Ø·ªúNG QUAN TR·ªåNG CHO FOCUS
                    "focus_score": focus_score,
                    "concentration_level": self._get_concentration_level(engagement),
                    "focus_duration": 45.0,
                
                    # D·ªØ li·ªáu kh√°c
                    "emotion": face.get("emotion"),
                    "emotion_confidence": face.get("emotion_confidence", 0.5),
                    "behavior_type": behavior_text,
                    "behavior_score": focus_score * 0.9,
                    "behavior_details": behavior_text,
                    "attendance_status": "present",
                    # üî¥ FIX: Datetime fields
                    "check_in_time": now.isoformat(),
                    "date": now.strftime("%Y-%m-%d"),
                    "class_name": "STEM 1",  # üî¥ TH√äM class_name c·ªë ƒë·ªãnh
                    "session_id": batch_data["session_id"],
                    "recorded_by": "AI Recognition System"
                }
                batch_data["data"].append(item)
            
            # üî¥ TH√äM: Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá th√¨ kh√¥ng g·ª≠i
            if not batch_data["data"]:
                return False
    
            # G·ª≠i batch ƒë·∫øn /api/ai/batch-process trong thread ri√™ng
            print("send detection batch")
            thread = threading.Thread(
                target=self._send_direct_batch,
                args=(batch_data,),
                daemon=True
            )
            thread.start()
            return True
    
        except Exception as e:
            print(f"Error in send_detection_batch: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _send_direct_batch(self, batch_data):
        """G·ª≠i batch tr·ª±c ti·∫øp ƒë·∫øn /api/ai/batch-process"""
        try:
            # üî¥ TH√äM: Ki·ªÉm tra n·∫øu batch r·ªóng
            if not batch_data.get("data"):
                return
            
            response = requests.post(
                self.batch_endpoint,
                json=batch_data,
                headers=self.headers,
                timeout=2
            )
        
            if response.status_code == 200:
                result = response.json()
                if result.get("status") == "success":
                    count = result.get("processed_count", 0)
                    # Log th√¥ng tin ID
                    if batch_data["data"]:
                        first_item = batch_data["data"][0]
                        name = first_item.get("student_name", "Unknown")
                        student_id = first_item.get("student_id", "N/A")
                        student_emotion = first_item.get("emotion")
                        student_behavior = first_item.get("behavior_type")
                        print(f"üì¶ Batch sent: {count} items | First: {name} ({student_id}) - {student_emotion} - {student_behavior}")
        except Exception as e:
            pass  # B·ªè qua l·ªói network
    
    # üî¥ TH√äM: H√†m debug ƒë·ªÉ xem mapping
    def debug_mapping(self):
        """Hi·ªÉn th·ªã th√¥ng tin mapping hi·ªán t·∫°i"""
        print("\n" + "="*80)
        print("üîç STUDENT ID MAPPING DEBUG")
        print("="*80)
        print(f"Total mappings: {len(self.student_name_to_id)}")
        
        # Hi·ªÉn th·ªã 10 mapping ƒë·∫ßu
        print("\nTop 10 mappings:")
        for i, (name, student_id) in enumerate(list(self.student_name_to_id.items())[:10]):
            print(f"{i+1:2d}. '{name}' -> {student_id}")
        
        if len(self.student_name_to_id) > 10:
            print(f"   ... and {len(self.student_name_to_id) - 10} more")
        
        print(f"\nReverse mappings: {len(self.student_id_to_name)}")
        print("="*80)

# ==================== GPU CONFIGURATION ====================
def setup_gpu():
    """C·∫•u h√¨nh v√† ki·ªÉm tra GPU chi ti·∫øt"""
    print("üîç Ki·ªÉm tra h·ªá th·ªëng GPU...")
    
    # Ki·ªÉm tra PyTorch CUDA
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(current_device)
            gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
            
            print(f"‚úÖ PyTorch GPU ƒë∆∞·ª£c h·ªó tr·ª£: {device_name}")
            print(f"üéØ S·ªë GPU: {gpu_count}")
            print(f"üíæ B·ªô nh·ªõ GPU: {gpu_memory:.1f} GB")
            
            # Thi·∫øt l·∫≠p GPU m·∫∑c ƒë·ªãnh
            torch.cuda.set_device(current_device)
            return True, 'cuda'
        else:
            print("‚ùå PyTorch kh√¥ng t√¨m th·∫•y GPU")
    except Exception as e:
        print(f"‚ùå L·ªói ki·ªÉm tra PyTorch GPU: {e}")
    
    print("üîß S·ª≠ d·ª•ng CPU mode - H·ªá th·ªëng v·∫´n ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
    return False, 'cpu'

def install_dependencies():
    """C√†i ƒë·∫∑t dependencies v·ªõi fix cho c√°c l·ªói"""
    print("üîß Ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...")
    
    # Danh s√°ch packages v·ªõi versions ·ªïn ƒë·ªãnh
    packages = [
        "torch>=2.0.0",
        "torchvision>=0.15.0", 
        "opencv-python>=4.8.0", 
        "matplotlib>=3.7.0",
        "scikit-learn>=1.3.0",
        "pillow>=10.0.0",
        "numpy==1.24.3",  # Fixed version for compatibility
        "insightface>=0.7.3",
        "deepface>=0.0.79",  # Version c≈© h∆°n, ·ªïn ƒë·ªãnh h∆°n
        "pandas>=2.0.0",
        "ultralytics==8.0.196",  # Version ·ªïn ƒë·ªãnh, tr√°nh l·ªói C3k2
        "requests>=2.31.0",
        "flask>=3.0.0",
        "flask-cors>=4.0.0",
        "scipy>=1.11.0",
        "pyyaml>=6.0",
    ]
    
    # Ki·ªÉm tra GPU ƒë·ªÉ quy·∫øt ƒë·ªãnh onnxruntime version
    gpu_available, _ = setup_gpu()
    if gpu_available:
        packages.append("onnxruntime-gpu>=1.16.0")
        print("üéØ S·∫Ω c√†i ƒë·∫∑t onnxruntime-gpu cho GPU")
    else:
        packages.append("onnxruntime>=1.16.0")
        print("üéØ S·∫Ω c√†i ƒë·∫∑t onnxruntime th∆∞·ªùng cho CPU")
    
    # Th·ª≠ c√†i ƒë·∫∑t t·ª´ng package
    for package in packages:
        try:
            # Extract package name (lo·∫°i b·ªè version specifier)
            pkg_name = package.split('>=')[0].split('==')[0]
            
            if pkg_name == "torch":
                import torch
                print(f"‚úÖ torch {torch.__version__} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            elif pkg_name == "torchvision":
                import torchvision
                print(f"‚úÖ torchvision {torchvision.__version__} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            elif pkg_name == "ultralytics":
                import ultralytics
                print(f"‚úÖ ultralytics {ultralytics.__version__} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            elif pkg_name == "onnxruntime-gpu":
                try:
                    import onnxruntime as ort
                    providers = ort.get_available_providers()
                    print(f"‚úÖ onnxruntime {ort.__version__} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
                    print(f"   Providers: {providers}")
                    continue
                except ImportError:
                    pass
            elif pkg_name == "onnxruntime":
                try:
                    import onnxruntime
                    print(f"‚úÖ onnxruntime {onnxruntime.__version__} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
                    continue
                except ImportError:
                    pass
            else:
                __import__(pkg_name.replace('-', '_'))
            print(f"‚úÖ {pkg_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        except ImportError:
            print(f"üì• ƒêang c√†i ƒë·∫∑t {package}...")
            try:
                # C√†i ƒë·∫∑t v·ªõi default pip
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                print(f"‚úÖ ƒê√£ c√†i ƒë·∫∑t {package}")
            except subprocess.CalledProcessError as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ c√†i ƒë·∫∑t {package}: {e}")
                print("üîÑ Th·ª≠ c√†i ƒë·∫∑t v·ªõi --user option...")
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--user"])
                    print(f"‚úÖ ƒê√£ c√†i ƒë·∫∑t {package} v·ªõi --user option")
                except subprocess.CalledProcessError as e2:
                    print(f"üö® Kh√¥ng th·ªÉ c√†i ƒë·∫∑t {package}: {e2}")
                    print("‚ö†Ô∏è Ti·∫øp t·ª•c v·ªõi package kh√°c...")

def check_system_capabilities():
    """Ki·ªÉm tra kh·∫£ nƒÉng h·ªá th·ªëng chi ti·∫øt"""
    print("\n" + "="*50)
    print("üîç KI·ªÇM TRA H·ªÜ TH·ªêNG CHI TI·∫æT")
    print("="*50)
    
    # Ki·ªÉm tra Python
    print(f"üêç Python Version: {sys.version}")
    
    # Ki·ªÉm tra OpenCV
    try:
        import cv2
        print(f"üì∑ OpenCV Version: {cv2.__version__}")
    except ImportError:
        print("‚ùå OpenCV ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    
    # Ki·ªÉm tra PyTorch
    try:
        import torch
        print(f"üî• PyTorch Version: {torch.__version__}")
        print(f"üîß CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print("üéØ PyTorch CUDA: S·∫¥N S√ÄNG")
            print(f"üîß GPU Name: {torch.cuda.get_device_name(0)}")
            print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("üéØ PyTorch CUDA: KH√îNG S·∫¥N S√ÄNG")
    except ImportError:
        print("‚ùå PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    
    # Ki·ªÉm tra Ultralytics (YOLO)
    try:
        import ultralytics
        print(f"üéØ Ultralytics Version: {ultralytics.__version__}")
    except ImportError:
        print("‚ùå Ultralytics ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    
    # Ki·ªÉm tra ONNX Runtime
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        print(f"üìä ONNX Runtime Version: {ort.__version__}")
        print(f"üîß Providers: {providers}")
    except ImportError:
        print("‚ùå ONNX Runtime ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    
    print("="*50)

# ==================== BEHAVIOR DETECTION - OPTIMIZED VERSION ====================

class BehaviorDetector:
    """Behavior detector t·ªëi ∆∞u v·ªõi temporal smoothing t√≠ch h·ª£p"""
    
    # COCO Keypoints indices
    KP = {
        'NOSE': 0,
        'LEFT_EYE': 1,
        'RIGHT_EYE': 2,
        'LEFT_EAR': 3,
        'RIGHT_EAR': 4,
        'LEFT_SHOULDER': 5,
        'RIGHT_SHOULDER': 6,
        'LEFT_ELBOW': 7,
        'RIGHT_ELBOW': 8,
        'LEFT_WRIST': 9,
        'RIGHT_WRIST': 10,
        'LEFT_HIP': 11,
        'RIGHT_HIP': 12
    }
    
    def __init__(self, device='cuda', history_length=15):
        """
        Args:
            device: 'cuda' or 'cpu'
            history_length: s·ªë frame l∆∞u l·∫°i ƒë·ªÉ temporal smoothing
        """
        self.device = device
        self.pose_model = None
        self.model_loaded = False
        
        # Temporal smoothing buffers v·ªõi tracking
        self.history_length = history_length
        self.behavior_history = defaultdict(lambda: deque(maxlen=history_length))
        self.person_tracking = {}  # tracking_id -> (center_x, center_y, timestamp)
        self.last_seen = {}  # tracking_id -> last seen timestamp
        
        # Configuration thresholds (T·∫§T C·∫¢ ƒê·ªÄU D√ôNG T·ªà L·ªÜ SO V·ªöI TH√ÇN NG∆Ø·ªúI)
        self.thresholds = {
            'hand_raised_ratio': 0.4,          # c·ªï tay cao h∆°n vai 40% chi·ªÅu cao th√¢n tr√™n
            'elbow_raised_ratio': 0.2,         # khu·ª∑u tay cao h∆°n vai 20% chi·ªÅu cao th√¢n tr√™n
            'writing_hand_below_shoulder': 0.1, # tay vi·∫øt th·∫•p h∆°n vai 10% chi·ªÅu cao th√¢n tr√™n
            'head_down_ratio': 0.15,           # m≈©i th·∫•p h∆°n m·∫Øt 15% chi·ªÅu cao th√¢n tr√™n
            'look_straight_thresh': 0.25,      # ng∆∞·ª°ng nh√¨n th·∫≥ng (t·ªâ l·ªá so v·ªõi kho·∫£ng c√°ch m·∫Øt)
            'keypoint_confidence': 0.3,
            'temporal_min_frames': 8,          # s·ªë frame t·ªëi thi·ªÉu ƒë·ªÉ smoothing
            'stabilization_confidence': 0.65,  # confidence ƒë·ªÉ thay ƒë·ªïi behavior stabilized
            'tracking_distance': 100,          # kho·∫£ng c√°ch tracking (pixels)
            'cleanup_timeout': 10.0           # th·ªùi gian x√≥a tracking c≈© (gi√¢y)
        }
        
        self._initialize_pose_detector()
    
    def _initialize_pose_detector(self):
        """Kh·ªüi t·∫°o YOLOv8 pose detector v·ªõi GPU t·ªëi ∆∞u"""
        try:
            import torch
            from ultralytics import YOLO
            
            print(f"üöÄ Initializing YOLOv8 Pose Detector on {self.device.upper()}...")
            
            # D√πng model nano ƒë·ªÉ nhanh nh·∫•t
            model_name = 'yolov8n-pose.pt'
            
            try:
                self.pose_model = YOLO(model_name)
                
                # Move to device v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
                if self.device == 'cuda' and torch.cuda.is_available():
                    self.pose_model.to('cuda')
                    
                    # Warm-up v·ªõi batch size nh·ªè
                    print("üî• Warming up GPU model...")
                    dummy_input = torch.randn(1, 3, 256, 256).to('cuda')
                    with torch.no_grad():
                        _ = self.pose_model(dummy_input)
                    torch.cuda.synchronize()
                    
                    print(f"‚úÖ Loaded {model_name} on GPU")
                else:
                    print(f"‚úÖ Loaded {model_name} on CPU")
                    self.device = 'cpu'
                
                self.model_loaded = True
                return True
                
            except Exception as e:
                print(f"‚ùå Failed to load {model_name}: {e}")
                return False
            
        except ImportError as e:
            print(f"‚ùå Ultralytics not installed: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Error initializing pose detector: {e}")
            return False
    
    def _get_keypoint(self, keypoints, idx):
        """L·∫•y keypoint v·ªõi confidence check - FIXED VERSION"""
        # Ki·ªÉm tra index h·ª£p l·ªá
        if idx >= len(keypoints):
            return None
        
        # Ki·ªÉm tra confidence
        if keypoints[idx][2] > self.thresholds['keypoint_confidence']:
            # Return [x, y] - ƒë√¢y l√† m·∫£ng numpy
            return keypoints[idx][:2].copy()  # D√πng .copy() ƒë·ªÉ tr√°nh reference
        
        return None
    
    def _get_body_scale(self, keypoints):
        """T√≠nh chi·ªÅu cao th√¢n tr√™n (shoulder to hip) - FIXED VERSION"""
        kp = self.KP
        
        # Th·ª≠ l·∫•y shoulder v√† hip b√™n tr√°i tr∆∞·ªõc
        left_shoulder = self._get_keypoint(keypoints, kp['LEFT_SHOULDER'])
        left_hip = self._get_keypoint(keypoints, kp['LEFT_HIP'])
        
        # S·ª¨A: Ki·ªÉm tra ƒë√∫ng c√°ch
        if left_shoulder is not None and left_hip is not None:
            return abs(left_hip[1] - left_shoulder[1])
        
        # Th·ª≠ b√™n ph·∫£i n·∫øu b√™n tr√°i kh√¥ng c√≥
        right_shoulder = self._get_keypoint(keypoints, kp['RIGHT_SHOULDER'])
        right_hip = self._get_keypoint(keypoints, kp['RIGHT_HIP'])
        
        if right_shoulder is not None and right_hip is not None:
            return abs(right_hip[1] - right_shoulder[1])
        
        # T√≠nh x·∫•p x·ªâ t·ª´ c√°c keypoints c√≥ s·∫µn
        shoulders = []
        hips = []
        
        for side in ['LEFT', 'RIGHT']:
            shoulder = self._get_keypoint(keypoints, kp[f'{side}_SHOULDER'])
            hip = self._get_keypoint(keypoints, kp[f'{side}_HIP'])
            
            # S·ª¨A: Ki·ªÉm tra ƒë√∫ng c√°ch
            if shoulder is not None:
                shoulders.append(shoulder)
            if hip is not None:
                hips.append(hip)
        
        if shoulders and hips:
            avg_shoulder_y = sum(s[1] for s in shoulders) / len(shoulders)
            avg_hip_y = sum(h[1] for h in hips) / len(hips)
            return abs(avg_hip_y - avg_shoulder_y)
        
        return None
    
    def _arm_raised(self, wrist, elbow, shoulder_y, body_scale):
        """Ki·ªÉm tra tay c√≥ gi∆° l√™n kh√¥ng (theo chu·∫©n l·ªõp h·ªçc)"""
        if wrist is None or elbow is None or body_scale is None:
            return False
        
        # ƒêi·ªÅu ki·ªán gi∆° tay:
        # 1. C·ªï tay cao h∆°n vai ‚â• 40% chi·ªÅu cao th√¢n tr√™n
        # 2. Khu·ª∑u tay cao h∆°n vai ‚â• 20% chi·ªÅu cao th√¢n tr√™n
        wrist_condition = wrist[1] < shoulder_y - (self.thresholds['hand_raised_ratio'] * body_scale)
        elbow_condition = elbow[1] < shoulder_y - (self.thresholds['elbow_raised_ratio'] * body_scale)
        
        return wrist_condition and elbow_condition
    
    def _detect_hand_raised(self, keypoints, body_scale):
        """Ph√°t hi·ªán gi∆° tay - phi√™n b·∫£n c·∫£i ti·∫øn cho l·ªõp h·ªçc"""
        kp = self.KP
        
        # L·∫•y keypoints
        lw = self._get_keypoint(keypoints, kp['LEFT_WRIST'])
        rw = self._get_keypoint(keypoints, kp['RIGHT_WRIST'])
        le = self._get_keypoint(keypoints, kp['LEFT_ELBOW'])
        re = self._get_keypoint(keypoints, kp['RIGHT_ELBOW'])
        ls = self._get_keypoint(keypoints, kp['LEFT_SHOULDER'])
        rs = self._get_keypoint(keypoints, kp['RIGHT_SHOULDER'])
        
        if body_scale is None:
            return None
        
        # T√≠nh t·ªça ƒë·ªô vai trung b√¨nh
        shoulders = []
        if ls is not None:
            shoulders.append(ls[1])
        if rs is not None:
            shoulders.append(rs[1])
        
        if not shoulders:
            return None
        
        shoulder_y = sum(shoulders) / len(shoulders)
        
        # Ki·ªÉm tra t·ª´ng tay
        left_raised = self._arm_raised(lw, le, shoulder_y, body_scale)
        right_raised = self._arm_raised(rw, re, shoulder_y, body_scale)
        
        # Ch·ªâ ph√°t hi·ªán gi∆° 1 tay (gi∆° 2 tay √≠t x·∫£y ra trong l·ªõp h·ªçc)
        if left_raised and not right_raised:
            return "raising_one_hand"
        elif right_raised and not left_raised:
            return "raising_one_hand"
        
        return None
    
    def _detect_writing(self, keypoints, body_scale):
        """Ph√°t hi·ªán vi·∫øt b√†i v·ªõi ng·ªØ c·∫£nh c√∫i ƒë·∫ßu + tay th·∫•p"""
        kp = self.KP
        
        if body_scale is None:
            return None
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán c√∫i ƒë·∫ßu
        nose = self._get_keypoint(keypoints, kp['NOSE'])
        le = self._get_keypoint(keypoints, kp['LEFT_EYE'])
        re = self._get_keypoint(keypoints, kp['RIGHT_EYE'])
        
        head_down = False
        if nose is not None and le is not None and re is not None:
            eyes_y = (le[1] + re[1]) / 2
            # M≈©i th·∫•p h∆°n m·∫Øt 15% chi·ªÅu cao th√¢n tr√™n
            if nose[1] > eyes_y + (self.thresholds['head_down_ratio'] * body_scale):
                head_down = True
        
        if not head_down:
            return None
        
        # Helper function ƒë·ªÉ ki·ªÉm tra t·ª´ng tay
        def check_writing_side(wrist, elbow, shoulder, hip):
            if wrist is None or elbow is None or shoulder is None:
                return False
            
            # 1. Tay vi·∫øt ph·∫£i th·∫•p h∆°n vai (tr√™n b√†n)
            if wrist[1] < shoulder[1] - (self.thresholds['writing_hand_below_shoulder'] * body_scale):
                return False
            
            # 2. G√≥c khu·ª∑u tay h·ª£p l√Ω
            def angle(A, B, C):
                BA = A - B
                BC = C - B
                cosang = np.dot(BA, BC) / (np.linalg.norm(BA) * np.linalg.norm(BC) + 1e-6)
                return np.degrees(np.arccos(np.clip(cosang, -1.0, 1.0)))
            
            elbow_angle = angle(np.array(shoulder), np.array(elbow), np.array(wrist))
            if not (70 < elbow_angle < 140):  # g√≥c h·ª£p l√Ω cho vi·∫øt
                return False
            
            # 3. Tay kh√¥ng qu√° xa th√¢n ng∆∞·ªùi
            if hip is not None:
                if abs(wrist[0] - hip[0]) > 0.8 * body_scale:
                    return False
            
            return True
        
        # Ki·ªÉm tra c·∫£ hai tay
        if check_writing_side(
            self._get_keypoint(keypoints, kp['LEFT_WRIST']),
            self._get_keypoint(keypoints, kp['LEFT_ELBOW']),
            self._get_keypoint(keypoints, kp['LEFT_SHOULDER']),
            self._get_keypoint(keypoints, kp['LEFT_HIP'])
        ):
            return "writing"
        
        if check_writing_side(
            self._get_keypoint(keypoints, kp['RIGHT_WRIST']),
            self._get_keypoint(keypoints, kp['RIGHT_ELBOW']),
            self._get_keypoint(keypoints, kp['RIGHT_SHOULDER']),
            self._get_keypoint(keypoints, kp['RIGHT_HIP'])
        ):
            return "writing"
        
        return None
    
    def _detect_look_direction(self, keypoints, body_scale):
        """Ph√°t hi·ªán h∆∞·ªõng nh√¨n d√πng t·ªâ l·ªá kho·∫£ng c√°ch m·∫Øt"""
        kp = self.KP
        
        nose = self._get_keypoint(keypoints, kp['NOSE'])
        le = self._get_keypoint(keypoints, kp['LEFT_EYE'])
        re = self._get_keypoint(keypoints, kp['RIGHT_EYE'])
        
        if nose is None or le is None or re is None:
            return "unknown"
        
        # T√≠nh t√¢m m·∫∑t
        face_center_x = (le[0] + re[0]) / 2
        
        # T√≠nh kho·∫£ng c√°ch gi·ªØa hai m·∫Øt
        eye_distance = abs(le[0] - re[0])
        if eye_distance < 1e-6:
            return "unknown"
        
        # T√≠nh ƒë·ªô l·ªách m≈©i so v·ªõi t√¢m m·∫∑t (chu·∫©n h√≥a theo kho·∫£ng c√°ch m·∫Øt)
        yaw = (nose[0] - face_center_x) / eye_distance
        
        # Ph√¢n lo·∫°i
        if abs(yaw) < self.thresholds['look_straight_thresh']:
            return "look_straight"
        else:
            return "look_around"
    
    def _determine_primary_behavior(self, hand_behavior, writing_behavior, look_behavior):
        """
        Priority c·∫£i ti·∫øn cho l·ªõp h·ªçc:
        1. Gi∆° tay
        2. Vi·∫øt b√†i
        3. Nh√¨n quanh (ch·ªâ khi KH√îNG vi·∫øt)
        4. Nh√¨n th·∫≥ng
        """
        if hand_behavior == "raising_one_hand":
            return hand_behavior
        
        if writing_behavior == "writing":
            return "writing"
        
        if look_behavior == "look_around" and writing_behavior != "writing":
            return "look_around"
        
        if look_behavior == "look_straight":
            return look_behavior
        
        return "unknown"
    
    def _assign_tracking_id(self, center_x, center_y, current_time):
        """G√°n ho·∫∑c t√¨m tracking ID d·ª±a tr√™n v·ªã tr√≠"""
        # T√¨m tracking ID g·∫ßn nh·∫•t
        closest_id = None
        min_distance = float('inf')
        
        for track_id, (last_x, last_y, last_time) in self.person_tracking.items():
            # Ch·ªâ x√©t trong 5 gi√¢y g·∫ßn nh·∫•t
            if current_time - last_time > 5.0:
                continue
            
            distance = np.sqrt((center_x - last_x)**2 + (center_y - last_y)**2)
            
            if distance < self.thresholds['tracking_distance'] and distance < min_distance:
                min_distance = distance
                closest_id = track_id
        
        # N·∫øu t√¨m th·∫•y, c·∫≠p nh·∫≠t
        if closest_id is not None:
            self.person_tracking[closest_id] = (center_x, center_y, current_time)
            self.last_seen[closest_id] = current_time
            return closest_id
        
        # T·∫°o ID m·ªõi
        new_id = len(self.person_tracking)
        self.person_tracking[new_id] = (center_x, center_y, current_time)
        self.last_seen[new_id] = current_time
        return new_id
    
    def _cleanup_old_tracking(self, current_time):
        """X√≥a tracking data c≈©"""
        ids_to_remove = []
        
        for track_id, last_time in self.last_seen.items():
            if current_time - last_time > self.thresholds['cleanup_timeout']:
                ids_to_remove.append(track_id)
        
        for track_id in ids_to_remove:
            self.person_tracking.pop(track_id, None)
            self.last_seen.pop(track_id, None)
            self.behavior_history.pop(track_id, None)
        
        if ids_to_remove:
            print(f"üßπ Cleaned up {len(ids_to_remove)} old tracking IDs")
    
    def _apply_stabilization(self, tracking_id, current_behavior, current_time):
        """√Åp d·ª•ng stabilization v·ªõi tracking ID c·ªë ƒë·ªãnh"""
        # Th√™m h√†nh vi hi·ªán t·∫°i v√†o l·ªãch s·ª≠
        self.behavior_history[tracking_id].append((current_behavior, current_time))
        
        # N·∫øu ch∆∞a ƒë·ªß frame, tr·∫£ v·ªÅ h√†nh vi hi·ªán t·∫°i
        if len(self.behavior_history[tracking_id]) < self.thresholds['temporal_min_frames']:
            return current_behavior
        
        # L·∫•y danh s√°ch behaviors g·∫ßn ƒë√¢y (c√≥ timestamp trong 3 gi√¢y)
        recent_behaviors = []
        for behavior, timestamp in self.behavior_history[tracking_id]:
            if current_time - timestamp <= 3.0:
                recent_behaviors.append(behavior)
        
        if not recent_behaviors:
            return current_behavior
        
        # T√¨m behavior ph·ªï bi·∫øn nh·∫•t
        behavior_counts = Counter(recent_behaviors)
        most_common_behavior, count = behavior_counts.most_common(1)[0]
        
        # T√≠nh confidence
        confidence = count / len(recent_behaviors)
        
        # ∆Øu ti√™n c√°c behaviors quan tr·ªçng
        important_behaviors = ['raising_one_hand', 'writing']
        
        # N·∫øu current l√† behavior quan tr·ªçng, ∆∞u ti√™n gi·ªØ l·∫°i
        if current_behavior in important_behaviors:
            if most_common_behavior not in important_behaviors:
                return current_behavior
        
        # Ch·ªâ thay ƒë·ªïi n·∫øu confidence ƒë·ªß cao
        if confidence >= self.thresholds['stabilization_confidence']:
            if most_common_behavior != current_behavior:
                print(f"üîÑ Stabilized: {current_behavior} -> {most_common_behavior} "
                      f"(conf: {confidence:.2f}, track_id: {tracking_id})")
            return most_common_behavior
        
        return current_behavior
    
    def detect_behavior(self, image, use_stabilization=True):
        """Nh·∫≠n di·ªán h√†nh vi v·ªõi stabilization t√≠ch h·ª£p"""
        if not self.model_loaded or self.pose_model is None:
            print("‚ö†Ô∏è Pose model not loaded")
            return []
        
        try:
            h, w = image.shape[:2]
            current_time = time.time()
            target_size = 320
            
            # Resize ·∫£nh ƒë·ªÉ tƒÉng t·ªëc x·ª≠ l√Ω
            if max(h, w) > 640:
                scale = target_size / max(h, w)
                new_h, new_w = int(h * scale), int(w * scale)
                image_resized = cv2.resize(image, (new_w, new_h))
            else:
                image_resized = image
            
            # Run inference
            results = self.pose_model(
                image_resized,
                conf=0.3,
                iou=0.45,
                imgsz=target_size,
                device=self.device,
                half=False,
                verbose=False,
                max_det=10
            )
            
            behaviors = []
            
            for result_idx, result in enumerate(results):
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints_data = result.keypoints.data
                    
                    # Convert to numpy
                    if hasattr(keypoints_data, 'cpu'):
                        keypoints_np = keypoints_data.cpu().numpy()
                    else:
                        keypoints_np = keypoints_data
                    
                    # X·ª≠ l√Ω shape c·ªßa keypoints
                    # keypoints_np c√≥ shape (num_people, num_keypoints, 3)
                    if len(keypoints_np.shape) != 3:
                        continue
                    
                    # Scale keypoints v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
                    if image_resized is not image:
                        scale_h = h / image_resized.shape[0]
                        scale_w = w / image_resized.shape[1]
                        keypoints_np[:, :, 0] *= scale_w
                        keypoints_np[:, :, 1] *= scale_h
                    
                    for person_idx, keypoints in enumerate(keypoints_np):
                        # ƒê·∫øm s·ªë keypoints visible
                        visible_kps = sum(1 for kp in keypoints 
                                        if kp[2] > self.thresholds['keypoint_confidence'])
                        
                        # B·ªè qua n·∫øu qu√° √≠t keypoints
                        if visible_kps < 6:
                            continue
                        
                        # T√≠nh body scale cho ng∆∞·ªùi n√†y
                        body_scale = self._get_body_scale(keypoints)
                        
                        # N·∫øu kh√¥ng c√≥ body_scale, b·ªè qua
                        if body_scale is None or body_scale < 10:
                            continue
                        
                        # Ph√°t hi·ªán c√°c h√†nh vi
                        hand_behavior = self._detect_hand_raised(keypoints, body_scale)
                        writing_behavior = self._detect_writing(keypoints, body_scale)
                        look_behavior = self._detect_look_direction(keypoints, body_scale)
                        
                        # X√°c ƒë·ªãnh h√†nh vi ch√≠nh
                        primary_behavior = self._determine_primary_behavior(
                            hand_behavior, writing_behavior, look_behavior
                        )
                        
                        # L·∫•y bounding box
                        bbox = None
                        if hasattr(result, 'boxes') and result.boxes is not None:
                            boxes_data = result.boxes.data
                            if person_idx < len(boxes_data):
                                box = boxes_data[person_idx]
                                if hasattr(box, 'cpu'):
                                    bbox = box[:4].cpu().numpy()
                                else:
                                    bbox = box[:4]
                                
                                # Scale bbox v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
                                if image_resized is not image and bbox is not None:
                                    bbox[0] *= scale_w
                                    bbox[1] *= scale_h
                                    bbox[2] *= scale_w
                                    bbox[3] *= scale_h
                        
                        # G√°n tracking ID n·∫øu c√≥ bbox
                        tracking_id = None
                        if bbox is not None:
                            x1, y1, x2, y2 = bbox.astype(int)
                            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
                            tracking_id = self._assign_tracking_id(center_x, center_y, current_time)
                            
                            # √Åp d·ª•ng stabilization n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
                            if use_stabilization and tracking_id is not None:
                                final_behavior = self._apply_stabilization(
                                    tracking_id, primary_behavior, current_time
                                )
                            else:
                                final_behavior = primary_behavior
                        else:
                            final_behavior = primary_behavior
                        
                        # T√≠nh confidence
                        confidence = min(0.95, visible_kps / 13)
                        
                        behaviors.append({
                            'behavior': final_behavior,
                            'raw_behaviors': {
                                'hand': hand_behavior,
                                'writing': writing_behavior,
                                'look': look_behavior
                            },
                            'confidence': float(confidence),
                            'bbox': bbox,
                            'tracking_id': tracking_id,
                            'person_idx': person_idx,
                            'visible_keypoints': visible_kps,
                            'body_scale': float(body_scale) if body_scale else None,
                            'timestamp': current_time,
                            'history_size': len(self.behavior_history.get(tracking_id, []))
                        })
                else:
                    print(f"üéØ Result {result_idx}: No keypoints detected")
            
            # Cleanup old tracking data
            self._cleanup_old_tracking(current_time)
            
            return behaviors
            
        except Exception as e:
            print(f"‚ùå Error in behavior detection: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def clear_history(self, tracking_id=None):
        """X√≥a l·ªãch s·ª≠ behavior v√† tracking"""
        if tracking_id:
            if tracking_id in self.behavior_history:
                self.behavior_history[tracking_id].clear()
            self.person_tracking.pop(track_id, None)
            self.last_seen.pop(track_id, None)
        else:
            self.behavior_history.clear()
            self.person_tracking.clear()
            self.last_seen.clear()
            print("üßπ Cleared all history and tracking data")
    
    def visualize(self, image, behaviors, show_raw=False, show_history=False, show_tracking=False):
        """Visualize behaviors on image"""
        visualized = image.copy()
        
        for behavior_data in behaviors:
            bbox = behavior_data.get('bbox')
            behavior = behavior_data.get('behavior')
            confidence = behavior_data.get('confidence', 0.5)
            tracking_id = behavior_data.get('tracking_id')
            
            if bbox is not None:
                x1, y1, x2, y2 = bbox.astype(int)
                
                # M√†u s·∫Øc theo behavior
                color_map = {
                    'raising_one_hand': (0, 255, 0),      # Xanh l√°
                    'writing': (255, 255, 0),            # V√†ng
                    'look_around': (255, 165, 0),        # Cam
                    'look_straight': (0, 255, 255),      # Cyan
                    'unknown': (150, 150, 150)           # X√°m
                }
                
                color = color_map.get(behavior, (200, 200, 200))
                
                # V·∫Ω bounding box
                cv2.rectangle(visualized, (x1, y1), (x2, y2), color, 2)
                
                # Chu·∫©n b·ªã text
                text_parts = []
                
                if show_tracking and tracking_id is not None:
                    text_parts.append(f"ID:{tracking_id}")
                
                text_parts.append(f"{behavior}")
                text_parts.append(f"{confidence:.1f}")
                
                if show_raw:
                    raw = behavior_data.get('raw_behaviors', {})
                    text_parts.append(f"H:{raw.get('hand') or '-'}")
                    text_parts.append(f"W:{raw.get('writing') or '-'}")
                    text_parts.append(f"L:{raw.get('look') or '-'}")
                
                if show_history:
                    hist_size = behavior_data.get('history_size', 0)
                    text_parts.append(f"H:{hist_size}")
                
                text = " ".join(text_parts)
                
                # Background cho text
                (text_width, text_height), baseline = cv2.getTextSize(
                    text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
                )
                
                # V·∫Ω background rectangle cho text
                cv2.rectangle(visualized, 
                            (x1, y1 - text_height - 10),
                            (x1 + text_width + 10, y1),
                            (0, 0, 0), -1)
                
                # V·∫Ω text
                cv2.putText(visualized, text, (x1 + 5, y1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        return visualized
        
# ==================== ATTENDANCE SYSTEM ====================
class AttendanceSystem:
    def __init__(self, csv_file="attendance.csv"):
        self.csv_file = csv_file
        self.backend_sender = EnhancedBackendDataSender()
        self.initialize_attendance_file()
    
    def initialize_attendance_file(self):
        """Kh·ªüi t·∫°o file ƒëi·ªÉm danh"""
        try:
            if not os.path.exists(self.csv_file):
                df = pd.DataFrame(columns=[
                    'Name', 'Date', 'Time', 'Emotion', 'Behavior', 'Confidence', 'Engagement', 'Concentration_Level'
                ])
                df.to_csv(self.csv_file, index=False)
                print(f"‚úÖ ƒê√£ t·∫°o file ƒëi·ªÉm danh: {self.csv_file}")
            else:
                df = pd.read_csv(self.csv_file)
                print(f"‚úÖ File ƒëi·ªÉm danh ƒë√£ t·ªìn t·∫°i: {len(df)} records")
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o file ƒëi·ªÉm danh: {str(e)}")
    
    def mark_attendance(self, name, emotion, emotion_confidence, behavior, engagement, concentration_level, confidence, bbox=None):
        """Queue d·ªØ li·ªáu ƒëi·ªÉm danh (async)"""
        try:
            now = datetime.now()
            date_str = now.strftime("%Y-%m-%d")
            time_str = now.strftime("%H:%M:%S")
            
            student_id = f"SV{hash(name) % 10000:04d}"
            
            student_data = {
                'name': name,
                'student_id': student_id,
                'db_student_id': self.backend_sender.get_student_id_from_name(name),
                'emotion': emotion,
                'emotion_confidence': emotion_confidence,
                'behavior': behavior,
                'engagement': engagement,
                'concentration_level': concentration_level,
                'confidence': confidence,
                'timestamp': now.isoformat()
            }
            
            # üî¥ THAY ƒê·ªîI: Queue t·∫•t c·∫£ d·ªØ li·ªáu async
            if self.backend_sender.is_connected:
                # Queue c√°c lo·∫°i d·ªØ li·ªáu
                self.backend_sender.queue_attendance_data(student_data)
                self.backend_sender.queue_behavior_data(student_data)
                self.backend_sender.queue_emotion_data(student_data)
                self.backend_sender.queue_engagement_data(student_data)
                
                # Log 1 l·∫ßn duy nh·∫•t
                print(f"üì• Queued: {name} ({emotion[:3]}, {behavior}, engagement: {engagement})")
            
            # L∆∞u v√†o file local (sync nh∆∞ng nhanh)
            try:
                df = pd.read_csv(self.csv_file)
            except:
                df = pd.DataFrame(columns=[
                    'Name', 'Date', 'Time', 'Emotion', 'Behavior', 'Confidence', 'Engagement', 'Concentration_Level'
                ])
            
            # Ki·ªÉm tra duplicate trong 2 ph√∫t
            two_minutes_ago = (datetime.now() - pd.Timedelta(minutes=2)).strftime("%H:%M:%S")
            recent_entries = df[
                (df['Name'] == name) & 
                (df['Date'] == date_str) & 
                (df['Time'] > two_minutes_ago)
            ]
            
            if len(recent_entries) == 0:
                new_entry = {
                    'Name': name,
                    'Date': date_str,
                    'Time': time_str,
                    'Emotion': emotion,
                    'Behavior': behavior,
                    'Confidence': f"{confidence:.4f}",
                    'Engagement': f"{engagement:.4f}",
                    'Concentration_Level': concentration_level
                }
                
                df = pd.concat([df, pd.DataFrame([new_entry])], ignore_index=True)
                df.to_csv(self.csv_file, index=False)
                
                return True
            else:
                return False
                
        except Exception as e:
            # Kh√¥ng log ƒë·ªÉ tr√°nh spam
            return False
    
    def view_attendance(self):
        """Xem l·ªãch s·ª≠ ƒëi·ªÉm danh"""
        try:
            if not os.path.exists(self.csv_file):
                print("üì≠ Ch∆∞a c√≥ file ƒëi·ªÉm danh")
                return
                
            df = pd.read_csv(self.csv_file)
            if len(df) > 0:
                print("\nüìä L·ªäCH S·ª¨ ƒêI·ªÇM DANH:")
                print("=" * 120)
                for _, row in df.iterrows():
                    print(f"üë§ {row['Name']} | üìÖ {row['Date']} | üïí {row['Time']} | üòä {row['Emotion']} | üéØ {row['Behavior']} | üìä {row['Engagement']} | üéØ {row['Concentration_Level']}")
                print("=" * 120)
                print(f"üìà T·ªïng s·ªë l∆∞·ª£t ƒëi·ªÉm danh: {len(df)}")
            else:
                print("üì≠ Ch∆∞a c√≥ d·ªØ li·ªáu ƒëi·ªÉm danh")
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file ƒëi·ªÉm danh: {str(e)}")

# ==================== EMOTION DETECTION - FIXED VERSION ====================
class EmotionDetector:
    def __init__(self, min_face_size=64, confidence_threshold=0.3):
        """
        Args:
            min_face_size: K√≠ch th∆∞·ªõc khu√¥n m·∫∑t t·ªëi thi·ªÉu (pixels)
            confidence_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu
        """
        self.min_face_size = min_face_size
        self.confidence_threshold = confidence_threshold
        self.logger = logging.getLogger(__name__)
        
        # Danh s√°ch c·∫£m x√∫c h·ªó tr·ª£ b·ªüi DeepFace
        self.supported_emotions = [
            'angry', 'disgust', 'fear', 'happy',
            'sad', 'surprise', 'neutral'
        ]
        
        # Ki·ªÉm tra DeepFace availability
        self.deepface_available = DEEPFACE_AVAILABLE
        
        if not self.deepface_available:
            print("‚ö†Ô∏è DeepFace kh√¥ng kh·∫£ d·ª•ng. Emotion detection s·∫Ω b·ªã gi·ªõi h·∫°n.")
    
    def detect_emotion(self, face_image, return_all=False):
        """
        Nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ khu√¥n m·∫∑t
        
        Args:
            face_image: ·∫¢nh khu√¥n m·∫∑t (BGR format)
            return_all: Tr·∫£ v·ªÅ t·∫•t c·∫£ emotions hay ch·ªâ dominant
        
        Returns:
            tuple: (dominant_emotion, confidence) ho·∫∑c dict c·ªßa t·∫•t c·∫£ emotions
        """
        if not self.deepface_available:
            # Fallback ƒë∆°n gi·∫£n: lu√¥n tr·∫£ v·ªÅ neutral
            if return_all:
                return {"neutral": 0.5}
            else:
                return "neutral", 0.5
        
        # Ki·ªÉm tra ·∫£nh ƒë·∫ßu v√†o
        if face_image is None or face_image.size == 0:
            if return_all:
                return {"neutral": 0.3}
            else:
                return "neutral", 0.3
        
        try:
            # Convert BGR to RGB
            face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
            
            # S·ª≠ d·ª•ng DeepFace v·ªõi c·∫•u h√¨nh ƒë∆°n gi·∫£n
            try:
                analysis = DeepFace.analyze(
                    img_path=face_rgb,
                    actions=['emotion'],
                    enforce_detection=False,  # Kh√¥ng b·∫Øt bu·ªôc ph·∫£i detect ƒë∆∞·ª£c face
                    detector_backend='opencv',
                    silent=True
                )
            except Exception as deepface_error:
                print(f"‚ö†Ô∏è DeepFace analysis error: {deepface_error}")
                # Fallback
                if return_all:
                    return {"neutral": 0.4}
                else:
                    return "neutral", 0.4
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(analysis, list) and len(analysis) > 0:
                analysis = analysis[0]
            
            if 'emotion' in analysis and 'dominant_emotion' in analysis:
                emotion_data = analysis['emotion']
                dominant_emotion = analysis['dominant_emotion']
                confidence = emotion_data.get(dominant_emotion, 50) / 100.0
                
                if return_all:
                    # Chu·∫©n h√≥a t·∫•t c·∫£ emotions v·ªÅ range 0-1
                    all_emotions = {}
                    for emotion, score in emotion_data.items():
                        all_emotions[emotion] = score / 100.0
                    return all_emotions
                else:
                    return dominant_emotion, confidence
            else:
                # No emotion data
                if return_all:
                    return {"neutral": 0.3}
                else:
                    return "neutral", 0.3
                
        except Exception as e:
            print(f"‚ùå Emotion detection error: {str(e)}")
            # Tr·∫£ v·ªÅ neutral nh∆∞ng v·ªõi confidence th·∫•p ƒë·ªÉ bi·∫øt c√≥ l·ªói
            if return_all:
                return {"neutral": 0.3}
            else:
                return "neutral", 0.3

# ==================== FACE RECOGNITION SYSTEM ====================
class CompleteRecognitionSystem:
    def __init__(self, model_name='buffalo_l', device='auto'):
        
        if device == 'auto':
            self.device = self._auto_detect_device()
        else:
            self.device = device
        
        print(f"üéØ System initialized on: {self.device.upper()}")
        self.model_name = model_name
        self.face_analyzer = None
        self.l2_normalizer = Normalizer('l2')
        
        # ==================== TH√äM: Kh·ªüi t·∫°o Emotion Detector ====================
        self.emotion_detector = EmotionDetector()
        print(f"üòä Emotion detector initialized")
                
        # S·ª≠ d·ª•ng StabilizedBehaviorDetectorGPU
        self.behavior_detector = BehaviorDetector(device)
        self.attendance_system = AttendanceSystem()
        self.backend_sender = self.attendance_system.backend_sender  # Use enhanced sender
        self.engagement_calculator = EngagementCalculator()  # üî¥ TH√äM: Engagement calculator
        
        # üî¥ TH√äM: Kh·ªüi t·∫°o tracking
        self.face_tracking_ids = {}
        self.last_tracking_cleanup = time.time()
        # Model
        self.svm_model = None

    def _auto_detect_device(self):
        """T·ª± ƒë·ªông ph√°t hi·ªán v√† ch·ªçn device t·ªët nh·∫•t"""
        print("üîç Auto-detecting best device...")
        
        # ∆Øu ti√™n 1: CUDA GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                
                print(f"‚úÖ Found GPU: {gpu_name}")
                print(f"üíæ GPU Memory: {gpu_memory:.1f} GB")
                
                # Ki·ªÉm tra memory ƒë·ªß kh√¥ng (√≠t nh·∫•t 2GB)
                if gpu_memory >= 2.0:
                    print("üéØ Using CUDA (GPU)")
                    return 'cuda'
                else:
                    print("‚ö†Ô∏è GPU memory too low (< 2GB), using CPU")
                    return 'cpu'
        except:
            pass
        
        # ∆Øu ti√™n 2: ONNX Runtime GPU
        try:
            import onnxruntime as ort
            providers = ort.get_available_providers()
            if 'CUDAExecutionProvider' in providers or 'TensorrtExecutionProvider' in providers:
                print("‚úÖ Found ONNX Runtime GPU provider")
                print("üéØ Using CUDA (ONNX)")
                return 'cuda'
        except:
            pass
        
        # M·∫∑c ƒë·ªãnh: CPU
        print("üîß Using CPU (no suitable GPU found)")
        return 'cpu'
    
    def _cleanup_old_tracking(self):
        """X√≥a tracking IDs c≈©"""
        current_time = time.time()
        if current_time - self.last_tracking_cleanup > 30:  # M·ªói 30 gi√¢y cleanup 1 l·∫ßn
            ids_to_remove = []
            for face_id, data in self.face_tracking_ids.items():
                if current_time - data.get('last_seen', 0) > 60:  # X√≥a sau 60 gi√¢y kh√¥ng th·∫•y
                    ids_to_remove.append(face_id)
            
            for face_id in ids_to_remove:
                del self.face_tracking_ids[face_id]
            
            self.last_tracking_cleanup = current_time
        
    def initialize_system(self):
        """Kh·ªüi t·∫°o to√†n b·ªô h·ªá th·ªëng v·ªõi device ph√π h·ª£p"""
        print(f"üöÄ Initializing system on {self.device.upper()}...")
        
        # 1. Kh·ªüi t·∫°o InsightFace v·ªõi device
        try:
            import insightface
            from insightface.app import FaceAnalysis
            
            print("üì• Loading InsightFace model...")
            self.face_analyzer = FaceAnalysis(name=self.model_name)
            
            # C·∫•u h√¨nh device cho InsightFace
            # -1 = CPU, 0 = GPU 0, 1 = GPU 1, ...
            ctx_id = -1 if self.device == 'cpu' else 0
            
            self.face_analyzer.prepare(
                ctx_id=ctx_id,
                det_size=(480, 480)  # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh cho performance
            )
            
            print(f"‚úÖ InsightFace ready on {'GPU' if ctx_id >= 0 else 'CPU'}")
            
        except Exception as e:
            print(f"‚ùå Failed to initialize InsightFace: {str(e)}")
            return False
        
        # 2. Emotion detector ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o trong __init__
        print(f"üòä Emotion detector ready")
        
        # 3. Test backend connection
        if self.backend_sender.is_connected:
            print("üåê Backend connection: CONNECTED")
        else:
            print("‚ö†Ô∏è Backend connection: DISCONNECTED")
        
        print("‚úÖ System initialization complete!")
        return True
    
    def get_device_info(self):
        """L·∫•y th√¥ng tin device chi ti·∫øt"""
        device_info = {
            'system_device': self.device,
            'components': {}
        }
        
        # InsightFace device info
        if self.face_analyzer:
            device_info['components']['insightface'] = {
                'status': 'loaded',
                'device': 'GPU' if hasattr(self.face_analyzer, 'ctx_id') and self.face_analyzer.ctx_id >= 0 else 'CPU'
            }
        
        # Behavior detector device info
        if hasattr(self.behavior_detector, 'device'):
            device_info['components']['behavior_detector'] = {
                'status': 'loaded',
                'device': self.behavior_detector.device.upper(),
                'model': 'yolov8n-pose'
            }
        
        # Emotion detector
        device_info['components']['emotion_detector'] = {
            'status': 'loaded',
            'device': 'CPU',  # DeepFace ch·ªâ ch·∫°y tr√™n CPU
            'backend': 'DeepFace' if DEEPFACE_AVAILABLE else 'Simple'
        }
        
        # GPU memory info n·∫øu c√≥
        try:
            import torch
            if torch.cuda.is_available():
                device_info['gpu'] = {
                    'name': torch.cuda.get_device_name(0),
                    'memory_total': f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB",
                    'memory_allocated': f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
                    'memory_cached': f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB"
                }
        except:
            pass
        
        return device_info

    def detect_faces(self, image):
        """Ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi InsightFace"""
        try:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            faces = self.face_analyzer.get(image_rgb)
            
            face_results = []
            for face in faces:
                bbox = face.bbox.astype(int)
                x1, y1, x2, y2 = bbox
                w = x2 - x1
                h = y2 - y1
                
                face_roi = image[y1:y2, x1:x2]
                if face_roi.size == 0:
                    continue
                
                embedding = face.normed_embedding
                
                # Nh·∫≠n di·ªán c·∫£m x√∫c - S·ª¨A L·ªñI: D√ôNG emotion_detector
                emotion, emotion_conf = self.emotion_detector.detect_emotion(face_roi)
                
                face_results.append({
                    'face_image': face_roi,
                    'bbox': (x1, y1, w, h),
                    'embedding': embedding,
                    'det_score': face.det_score,
                    'landmarks': face.kps if hasattr(face, 'kps') else None,
                    'emotion': emotion,
                    'emotion_confidence': emotion_conf
                })
            
            return face_results
            
        except Exception as e:
            print(f"‚ùå L·ªói detect faces: {str(e)}")
            return []

    def extract_features(self, face_data):
        """Tr√≠ch xu·∫•t features t·ª´ khu√¥n m·∫∑t"""
        try:
            embedding = face_data['embedding']
            embedding = embedding.reshape(1, -1)
            features_normalized = self.l2_normalizer.transform(embedding)
            return features_normalized[0]
        except Exception as e:
            print(f"‚ùå L·ªói extract features: {str(e)}")
            return None

    def train_face_recognition(self, database_path="database"):
        """Train h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t"""
        if not os.path.exists(database_path):
            print(f"‚ùå Th∆∞ m·ª•c database kh√¥ng t·ªìn t·∫°i: {database_path}")
            return False
        
        database = {}
        features_list = []
        labels_list = []
        
        print("üìÅ ƒêang x·ª≠ l√Ω database...")
        
        persons = [p for p in os.listdir(database_path) if os.path.isdir(os.path.join(database_path, p))]
        if len(persons) < 1:
            print("‚ùå Kh√¥ng c√≥ ng∆∞·ªùi n√†o trong database!")
            return False
        
        for person_name in persons:
            person_path = os.path.join(database_path, person_name)
            print(f"üë§ ƒêang x·ª≠ l√Ω: {person_name}")
            person_features = []
            
            image_files = [f for f in os.listdir(person_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            
            for image_file in image_files:
                image_path = os.path.join(person_path, image_file)
                image = cv2.imread(image_path)
                if image is None:
                    continue
                
                face_results = self.detect_faces(image)
                for face_data in face_results:
                    features = self.extract_features(face_data)
                    if features is not None:
                        person_features.append(features)
                        features_list.append(features)
                        labels_list.append(person_name)
            
            if person_features:
                database[person_name] = person_features
                print(f"  ‚ûï {person_name}: {len(person_features)} khu√¥n m·∫∑t")
        
        if len(features_list) == 0:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ train!")
            return False
        
        print(f"\nüìä Th·ªëng k√™ database:")
        print(f"üë• S·ªë ng∆∞·ªùi: {len(database)}")
        print(f"üñºÔ∏è T·ªïng khu√¥n m·∫∑t: {len(features_list)}")
        
        # Train SVM model
        print("\nüéØ ƒêang train SVM model...")
        self.svm_model = SVC(kernel='linear', probability=True, random_state=42)
        self.svm_model.fit(features_list, labels_list)
        
        accuracy = accuracy_score(labels_list, self.svm_model.predict(features_list))
        print(f"‚úÖ Training ho√†n t·∫•t! Accuracy: {accuracy:.4f}")
        
        # L∆∞u model
        with open("face_recognition_model.pkl", 'wb') as f:
            pickle.dump(self.svm_model, f)
        
        with open("face_database.pkl", 'wb') as f:
            pickle.dump({
                'database': database,
                'features': features_list,
                'labels': labels_list
            }, f)
        
        print("üíæ ƒê√£ l∆∞u model v√† database")
        return True

    def load_trained_model(self):
        """Load model ƒë√£ train v·ªõi fix numpy version"""
        try:
            print("üìÇ ƒêang load trained model...")
            
            if not os.path.exists("face_recognition_model.pkl"):
                print("‚ùå Kh√¥ng t√¨m th·∫•y file model. Vui l√≤ng train model tr∆∞·ªõc.")
                return False
            
            # Th·ª≠ load v·ªõi pickle
            import pickle
            
            try:
                with open("face_recognition_model.pkl", 'rb') as f:
                    self.svm_model = pickle.load(f)
                print("‚úÖ ƒê√£ load model th√†nh c√¥ng")
            except Exception as e:
                print(f"‚ùå L·ªói load model: {e}")
                
                # Th·ª≠ v·ªõi encoding kh√°c
                try:
                    with open("face_recognition_model.pkl", 'rb') as f:
                        self.svm_model = pickle.load(f, encoding='latin1')
                    print("‚úÖ ƒê√£ load model v·ªõi encoding latin1")
                except:
                    print("‚ùå Kh√¥ng th·ªÉ load model v·ªõi b·∫•t k·ª≥ encoding n√†o")
                    return False
            
            if hasattr(self.svm_model, 'classes_'):
                print(f"‚úÖ ƒê√£ load trained model - {len(self.svm_model.classes_)} classes")
                return True
            else:
                print("‚ö†Ô∏è Model ƒë∆∞·ª£c load nh∆∞ng kh√¥ng c√≥ classes")
                return False
            
        except Exception as e:
            print(f"‚ùå L·ªói load model: {str(e)}")
            return False

    def recognize_face(self, face_data, threshold=0.4):
        """Nh·∫≠n di·ªán khu√¥n m·∫∑t"""
        if not hasattr(self, 'svm_model') or self.svm_model is None:
            return "Unknown", 0.0
        
        features = self.extract_features(face_data)
        if features is None:
            return "Unknown", 0.0
        
        try:
            probabilities = self.svm_model.predict_proba([features])[0]
            max_prob = np.max(probabilities)
            predicted_class = self.svm_model.classes_[np.argmax(probabilities)]
            
            if max_prob < threshold:
                return "Unknown", max_prob
            else:
                return predicted_class, max_prob
        except:
            return "Unknown", 0.0

    def _match_face_to_behavior(self, face_data, behavior_results):
        """Matching v·ªõi tracking ID ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n - DEBUG VERSION"""
        
        face_bbox = face_data['bbox']
        x, y, w, h = face_bbox
        face_center_x = x + w/2
        face_center_y = y + h/2
        
        
        # TH√äM: Kh·ªüi t·∫°o face_tracking_ids n·∫øu ch∆∞a c√≥
        if not hasattr(self, 'face_tracking_ids'):
            self.face_tracking_ids = {}
        
        # T√¨m tracking ID cho face n√†y (n·∫øu c√≥)
        face_id = self._assign_face_id(face_bbox, self.face_tracking_ids)
        
        # C·∫≠p nh·∫≠t tracking
        self.face_tracking_ids[face_id] = {
            'bbox': face_bbox,
            'last_seen': time.time()
        }
        
        best_match = {'type': 'normal', 'confidence': 0.8, 'distance': float('inf')}
        
        # N·∫øu face c√≥ tracking ID, ∆∞u ti√™n matching v·ªõi behavior c√≥ c√πng ID
        for behavior_idx, behavior in enumerate(behavior_results):
            if behavior['bbox'] is not None:
                try:
                    bx1, by1, bx2, by2 = behavior['bbox'].astype(int)
                    behavior_center_x = (bx1 + bx2) / 2
                    behavior_center_y = (by1 + by2) / 2
                    
                    # T√≠nh kho·∫£ng c√°ch Euclid
                    distance = np.sqrt((face_center_x - behavior_center_x)**2 + (face_center_y - behavior_center_y)**2)
                    
                    # T√≠nh IoU (Intersection over Union)
                    intersection_x1 = max(x, bx1)
                    intersection_y1 = max(y, by1)
                    intersection_x2 = min(x + w, bx2)
                    intersection_y2 = min(y + h, by2)
                    
                    if intersection_x2 > intersection_x1 and intersection_y2 > intersection_y1:
                        intersection_area = (intersection_x2 - intersection_x1) * (intersection_y2 - intersection_y1)
                        face_area = w * h
                        behavior_area = (bx2 - bx1) * (by2 - by1)
                        union_area = face_area + behavior_area - intersection_area
                        
                        iou = intersection_area / union_area if union_area > 0 else 0
                        
                    else:
                        iou = 0
                    
                    if distance < best_match['distance']:
                        best_match = {
                            'type': behavior['behavior'],
                            'confidence': min(0.9, max(0.7, 1 - distance/300)),
                            'distance': distance,
                            'iou': iou
                        }
                        
                except Exception as e:
                    continue
            else:
                print(f"  Behavior {behavior_idx}: No bbox")
        
        return best_match

    def _assign_face_id(self, face_bbox, face_tracking_ids):
        """G√°n tracking ID cho face d·ª±a tr√™n bbox v√† tracking system"""
        x, y, w, h = face_bbox
        face_center_x = x + w/2
        face_center_y = y + h/2
        
        # T√¨m ID g·∫ßn nh·∫•t
        best_id = None
        min_distance = float('inf')
        
        for face_id, bbox_data in face_tracking_ids.items():
            if isinstance(bbox_data, dict) and 'bbox' in bbox_data:
                tracked_bbox = bbox_data['bbox']
                tracked_x, tracked_y, tracked_w, tracked_h = tracked_bbox
                tracked_center_x = tracked_x + tracked_w/2
                tracked_center_y = tracked_y + tracked_h/2
                
                distance = np.sqrt((face_center_x - tracked_center_x)**2 + 
                                  (face_center_y - tracked_center_y)**2)
                
                # N·∫øu kho·∫£ng c√°ch < 100 pixels, coi nh∆∞ c√πng m·ªôt ng∆∞·ªùi
                if distance < 100 and distance < min_distance:
                    min_distance = distance
                    best_id = face_id
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o ID m·ªõi
        if best_id is None:
            best_id = len(face_tracking_ids)
        
        return best_id

    def process_frame_with_engagement(self, frame, face_results, behavior_results):
        """X·ª≠ l√Ω frame v·ªõi t√≠nh to√°n engagement n√¢ng cao"""
        student_data_list = []
        
        for i, face_data in enumerate(face_results):
            bbox = face_data['bbox']
            x, y, w, h = bbox
            emotion = face_data['emotion']
            emotion_conf = face_data['emotion_confidence']
            
            if hasattr(self, 'svm_model') and self.svm_model:
                name, confidence = self.recognize_face(face_data)
            else:
                name, confidence = "Unknown", 0.0
            
            # Gh√©p v·ªõi h√†nh vi
            matched_behavior = self._match_face_to_behavior(face_data, behavior_results)
            behavior = matched_behavior['type']
            behavior_confidence = matched_behavior['confidence']
            
            # T√≠nh engagement score
            engagement_result = self.engagement_calculator.calculate_engagement(
                student_id=f"{name}_{i}",
                emotion=emotion,
                emotion_confidence=emotion_conf,
                behavior=behavior,
                behavior_confidence=behavior_confidence,
                bbox=(x, y, w, h)
            )
            
            student_data = {
                'id': i + 1,
                'name': name,
                'emotion': emotion,
                'emotion_confidence': emotion_conf,
                'behavior': behavior,
                'engagement': engagement_result['engagement_score'],  # NEW
                'concentration_level': engagement_result['concentration_level'],  # NEW
                'bbox': {'x': int(x), 'y': int(y), 'width': int(w), 'height': int(h)},
                'face_confidence': confidence,
                'engagement_details': engagement_result  # Chi ti·∫øt t√≠nh to√°n
            }
            
            student_data_list.append(student_data)
        
        return student_data_list

    def get_class_engagement_report(self):
        """L·∫•y b√°o c√°o engagement cho c·∫£ l·ªõp"""
        if hasattr(self, 'last_detection_results'):
            return self.engagement_calculator.get_engagement_report(self.last_detection_results)
        return None

    def _get_engagement_color(self, score):
        """L·∫•y m√†u d·ª±a tr√™n engagement score"""
        return self.engagement_calculator._get_engagement_color(score)

# ==================== FLASK API ENDPOINTS ====================
# ==================== VIDEO STREAM ENDPOINT ====================
@app.route('/video_feed')
def video_feed():
    """Endpoint stream video MJPEG - CH·ªà CAMERA TH∆Ø·ªúNG"""
    return Response(generate_mjpeg(),
                    mimetype='multipart/x-mixed-replace; boundary=frame',
                    headers={
                        'Cache-Control': 'no-cache, no-store, must-revalidate',
                        'Pragma': 'no-cache',
                        'Expires': '0'
                    })

@app.route('/')
def index():
    """Trang ch√≠nh v·ªõi video stream camera th∆∞·ªùng"""
    html = '''
    <!DOCTYPE html>
    <html>
    <head>
        <title>Camera Live Stream</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: #1a1a2e;
                color: white;
                min-height: 100vh;
                display: flex;
                flex-direction: column;
            }
            
            .header {
                text-align: center;
                padding: 20px;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            }
            
            .header h1 {
                font-size: 2.5rem;
                margin-bottom: 10px;
                background: linear-gradient(45deg, #fff, #e0e0e0);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
            }
            
            .container {
                flex: 1;
                display: flex;
                flex-direction: column;
                align-items: center;
                padding: 20px;
                max-width: 1200px;
                margin: 0 auto;
                width: 100%;
            }
            
            .video-container {
                width: 100%;
                max-width: 800px;
                background: #16213e;
                border-radius: 20px;
                overflow: hidden;
                box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
                margin-bottom: 30px;
                position: relative;
                border: 3px solid #4cc9f0;
            }
            
            .video-container::before {
                content: '';
                position: absolute;
                top: -2px;
                left: -2px;
                right: -2px;
                bottom: -2px;
                background: linear-gradient(45deg, #4cc9f0, #4361ee, #3a0ca3, #7209b7);
                border-radius: 22px;
                z-index: -1;
                animation: border-glow 3s ease-in-out infinite alternate;
            }
            
            @keyframes border-glow {
                0% {
                    opacity: 0.5;
                }
                100% {
                    opacity: 1;
                }
            }
            
            .video-header {
                background: rgba(0, 0, 0, 0.7);
                padding: 15px 20px;
                display: flex;
                justify-content: space-between;
                align-items: center;
                border-bottom: 2px solid #4cc9f0;
            }
            
            .video-header h3 {
                display: flex;
                align-items: center;
                gap: 10px;
                font-size: 1.2rem;
            }
            
            .status-indicator {
                display: flex;
                align-items: center;
                gap: 8px;
            }
            
            .status-dot {
                width: 12px;
                height: 12px;
                border-radius: 50%;
                background: #4CAF50;
                animation: pulse 2s infinite;
            }
            
            .status-dot.offline {
                background: #f44336;
                animation: none;
            }
            
            @keyframes pulse {
                0%, 100% {
                    opacity: 1;
                    transform: scale(1);
                }
                50% {
                    opacity: 0.7;
                    transform: scale(1.1);
                }
            }
            
            #videoStream {
                width: 100%;
                display: block;
                background: #000;
                min-height: 480px;
                object-fit: cover;
            }
            
            .controls {
                display: flex;
                gap: 15px;
                flex-wrap: wrap;
                justify-content: center;
                margin: 20px 0;
            }
            
            .btn {
                padding: 12px 24px;
                border: none;
                border-radius: 50px;
                font-size: 1rem;
                font-weight: 600;
                cursor: pointer;
                transition: all 0.3s ease;
                display: flex;
                align-items: center;
                gap: 8px;
                text-decoration: none;
            }
            
            .btn-primary {
                background: linear-gradient(135deg, #4cc9f0, #4361ee);
                color: white;
                box-shadow: 0 4px 15px rgba(76, 201, 240, 0.4);
            }
            
            .btn-primary:hover {
                transform: translateY(-2px);
                box-shadow: 0 6px 20px rgba(76, 201, 240, 0.6);
            }
            
            .btn-secondary {
                background: rgba(255, 255, 255, 0.1);
                color: white;
                border: 2px solid #4cc9f0;
            }
            
            .btn-secondary:hover {
                background: rgba(76, 201, 240, 0.2);
                transform: translateY(-2px);
            }
            
            .stats {
                display: flex;
                gap: 30px;
                margin-top: 20px;
                flex-wrap: wrap;
                justify-content: center;
            }
            
            .stat-box {
                background: rgba(255, 255, 255, 0.05);
                padding: 20px;
                border-radius: 15px;
                min-width: 200px;
                text-align: center;
                border: 1px solid rgba(76, 201, 240, 0.2);
                transition: transform 0.3s ease;
            }
            
            .stat-box:hover {
                transform: translateY(-5px);
                border-color: #4cc9f0;
            }
            
            .stat-value {
                font-size: 2rem;
                font-weight: bold;
                background: linear-gradient(45deg, #4cc9f0, #4361ee);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                margin: 10px 0;
            }
            
            .stat-label {
                color: #a0a0a0;
                font-size: 0.9rem;
                text-transform: uppercase;
                letter-spacing: 1px;
            }
            
            .info-panel {
                background: rgba(22, 33, 62, 0.8);
                padding: 20px;
                border-radius: 15px;
                margin-top: 30px;
                border: 1px solid rgba(76, 201, 240, 0.3);
                width: 100%;
                max-width: 800px;
            }
            
            .info-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                gap: 20px;
                margin-top: 15px;
            }
            
            .info-item {
                padding: 10px;
                background: rgba(0, 0, 0, 0.2);
                border-radius: 8px;
                border-left: 4px solid #4cc9f0;
            }
            
            .footer {
                text-align: center;
                padding: 20px;
                margin-top: 40px;
                color: #a0a0a0;
                font-size: 0.9rem;
                border-top: 1px solid rgba(255, 255, 255, 0.1);
            }
            
            @media (max-width: 768px) {
                .container {
                    padding: 10px;
                }
                
                .video-container {
                    border-radius: 10px;
                }
                
                .header h1 {
                    font-size: 1.8rem;
                }
                
                .btn {
                    padding: 10px 20px;
                    font-size: 0.9rem;
                }
                
                .stats {
                    gap: 15px;
                }
                
                .stat-box {
                    min-width: 150px;
                    padding: 15px;
                }
            }
            
            .loading {
                display: none;
                position: fixed;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background: rgba(0, 0, 0, 0.8);
                z-index: 1000;
                justify-content: center;
                align-items: center;
                flex-direction: column;
            }
            
            .loading.show {
                display: flex;
            }
            
            .spinner {
                width: 50px;
                height: 50px;
                border: 5px solid rgba(255, 255, 255, 0.1);
                border-top: 5px solid #4cc9f0;
                border-radius: 50%;
                animation: spin 1s linear infinite;
                margin-bottom: 20px;
            }
            
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
        </style>
    </head>
    <body>
        <!-- Header -->
        <div class="header">
            <h1>üìπ Camera Live Stream</h1>
            <p>Real-time camera feed without AI processing</p>
        </div>
        
        <!-- Loading Overlay -->
        <div class="loading" id="loading">
            <div class="spinner"></div>
            <p>Connecting to camera...</p>
        </div>
        
        <!-- Main Container -->
        <div class="container">
            <!-- Video Container -->
            <div class="video-container">
                <div class="video-header">
                    <h3>
                        <span>üî¥ LIVE</span>
                        <span>|</span>
                        <span>Camera Feed</span>
                    </h3>
                    <div class="status-indicator">
                        <div class="status-dot" id="statusDot"></div>
                        <span id="statusText">Connecting...</span>
                    </div>
                </div>
                <img id="videoStream" src="/video_feed" alt="Live Camera Stream">
            </div>
            
            <!-- Controls -->
            <div class="controls">
                <button class="btn btn-primary" onclick="refreshStream()">
                    <span>üîÑ</span>
                    Refresh Stream
                </button>
                <button class="btn btn-secondary" onclick="toggleFullscreen()">
                    <span>üì∫</span>
                    Fullscreen
                </button>
                <button class="btn btn-secondary" onclick="captureFrame()">
                    <span>üì∏</span>
                    Capture Photo
                </button>
                <a href="/video_feed" target="_blank" class="btn btn-secondary">
                    <span>üîó</span>
                    Direct Stream Link
                </a>
            </div>
            
            <!-- Statistics -->
            <div class="stats">
                <div class="stat-box">
                    <div class="stat-label">Stream Status</div>
                    <div class="stat-value" id="streamStatus">Live</div>
                    <div>Camera Feed Active</div>
                </div>
                
                <div class="stat-box">
                    <div class="stat-label">Resolution</div>
                    <div class="stat-value">640x480</div>
                    <div>Video Quality</div>
                </div>
                
                <div class="stat-box">
                    <div class="stat-label">Frame Rate</div>
                    <div class="stat-value" id="fpsCounter">30</div>
                    <div>Frames Per Second</div>
                </div>
            </div>
            
            <!-- Information Panel -->
            <div class="info-panel">
                <h3>‚ÑπÔ∏è Stream Information</h3>
                <div class="info-grid">
                    <div class="info-item">
                        <strong>Server URL:</strong>
                        <div style="word-break: break-all; color: #4cc9f0; margin-top: 5px;">
                            http://localhost:5000
                        </div>
                    </div>
                    
                    <div class="info-item">
                        <strong>Stream Endpoint:</strong>
                        <div style="word-break: break-all; color: #4cc9f0; margin-top: 5px;">
                            /video_feed
                        </div>
                    </div>
                    
                    <div class="info-item">
                        <strong>Connection Type:</strong>
                        <div style="color: #4cc9f0; margin-top: 5px;">
                            MJPEG Stream
                        </div>
                    </div>
                    
                    <div class="info-item">
                        <strong>Last Updated:</strong>
                        <div style="color: #4cc9f0; margin-top: 5px;" id="lastUpdate">
                            Just now
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Footer -->
        <div class="footer">
            <p>Camera Stream System | Real-time Video Feed</p>
            <p>Server running on http://localhost:5000</p>
        </div>
        
        <script>
            // DOM Elements
            const videoStream = document.getElementById('videoStream');
            const statusDot = document.getElementById('statusDot');
            const statusText = document.getElementById('statusText');
            const streamStatus = document.getElementById('streamStatus');
            const fpsCounter = document.getElementById('fpsCounter');
            const lastUpdate = document.getElementById('lastUpdate');
            const loading = document.getElementById('loading');
            
            // Variables
            let frameCount = 0;
            let lastTime = Date.now();
            let isConnected = false;
            let refreshInterval;
            
            // Auto-refresh stream to avoid cache
            function refreshStream() {
                console.log('üîÑ Refreshing stream...');
                const timestamp = Date.now();
                videoStream.src = `/video_feed?t=${timestamp}`;
                updateLastUpdate();
                showLoading();
                
                // Hide loading after 2 seconds
                setTimeout(() => {
                    hideLoading();
                }, 2000);
            }
            
            // Toggle fullscreen
            function toggleFullscreen() {
                if (!document.fullscreenElement) {
                    videoStream.requestFullscreen().catch(err => {
                        console.log('Fullscreen error:', err);
                    });
                } else {
                    document.exitFullscreen();
                }
            }
            
            // Capture frame
            function captureFrame() {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                
                canvas.width = videoStream.videoWidth || 640;
                canvas.height = videoStream.videoHeight || 480;
                
                ctx.drawImage(videoStream, 0, 0, canvas.width, canvas.height);
                
                const link = document.createElement('a');
                link.download = `capture_${Date.now()}.jpg`;
                link.href = canvas.toDataURL('image/jpeg', 0.9);
                link.click();
                
                alert('üì∏ Photo captured!');
            }
            
            // Update connection status
            function updateStatus(connected) {
                isConnected = connected;
                
                if (connected) {
                    statusDot.classList.remove('offline');
                    statusText.textContent = 'Connected';
                    streamStatus.textContent = 'Live';
                    streamStatus.style.color = '#4CAF50';
                } else {
                    statusDot.classList.add('offline');
                    statusText.textContent = 'Disconnected';
                    streamStatus.textContent = 'Offline';
                    streamStatus.style.color = '#f44336';
                }
            }
            
            // Calculate FPS
            function calculateFPS() {
                frameCount++;
                const now = Date.now();
                const delta = now - lastTime;
                
                if (delta >= 1000) { // Update every second
                    const fps = Math.round((frameCount * 1000) / delta);
                    fpsCounter.textContent = fps;
                    frameCount = 0;
                    lastTime = now;
                }
                
                requestAnimationFrame(calculateFPS);
            }
            
            // Update last update time
            function updateLastUpdate() {
                const now = new Date();
                const timeString = now.toLocaleTimeString('en-US', {
                    hour12: true,
                    hour: '2-digit',
                    minute: '2-digit',
                    second: '2-digit'
                });
                lastUpdate.textContent = `${timeString}`;
            }
            
            // Show loading
            function showLoading() {
                loading.classList.add('show');
            }
            
            // Hide loading
            function hideLoading() {
                loading.classList.remove('show');
            }
            
            // Check stream health
            async function checkStreamHealth() {
                try {
                    const response = await fetch('/api/health');
                    const data = await response.json();
                    
                    updateStatus(data.status === 'healthy');
                    
                    // If disconnected, try to reconnect
                    if (!data.status === 'healthy' && isConnected) {
                        console.log('Stream disconnected, attempting to reconnect...');
                        refreshStream();
                    }
                } catch (error) {
                    console.log('Health check failed:', error);
                    updateStatus(false);
                }
            }
            
            // Handle stream errors
            videoStream.onerror = function() {
                console.log('Stream error occurred');
                updateStatus(false);
                refreshStream();
            };
            
            videoStream.onload = function() {
                console.log('Stream loaded successfully');
                updateStatus(true);
                hideLoading();
            };
            
            // Initialize
            function init() {
                // Start FPS calculation
                calculateFPS();
                
                // Initial refresh
                refreshStream();
                
                // Start health checks every 5 seconds
                setInterval(checkStreamHealth, 5000);
                
                // Auto-refresh every 30 seconds to prevent timeout
                refreshInterval = setInterval(refreshStream, 30000);
                
                // Initial status check
                checkStreamHealth();
                
                // Keyboard shortcuts
                document.addEventListener('keydown', (e) => {
                    // Space to refresh
                    if (e.code === 'Space') {
                        e.preventDefault();
                        refreshStream();
                    }
                    
                    // F for fullscreen
                    if (e.code === 'KeyF') {
                        e.preventDefault();
                        toggleFullscreen();
                    }
                    
                    // C for capture
                    if (e.code === 'KeyC') {
                        e.preventDefault();
                        captureFrame();
                    }
                });
                
                console.log('üé• Camera stream system initialized');
            }
            
            // Start when page loads
            window.addEventListener('load', init);
            
            // Cleanup on page unload
            window.addEventListener('beforeunload', () => {
                if (refreshInterval) {
                    clearInterval(refreshInterval);
                }
            });
        </script>
    </body>
    </html>
    '''
    return html

@app.route('/api/capture', methods=['POST'])
def capture_frame():
    """Ch·ª•p frame hi·ªán t·∫°i"""
    global camera_manager
    
    try:
        frame = camera_manager.get_latest_frame()
        if frame is not None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"capture_{timestamp}.jpg"
            cv2.imwrite(filename, frame)
            
            return jsonify({
                'status': 'success',
                'message': f'Frame captured: {filename}',
                'filename': filename
            })
        
        return jsonify({'status': 'error', 'message': 'No frame available'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)})
    
@app.route('/api/camera/start', methods=['POST'])
def start_camera():
    """Kh·ªüi ƒë·ªông camera"""
    global camera_manager
    
    try:
        camera_index = request.json.get('camera_index', 0)
        camera_manager = CameraManager(camera_index=camera_index)
        
        if camera_manager.start():
            return jsonify({
                'status': 'success',
                'message': f'Camera {camera_index} started',
                'camera_index': camera_index
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'Cannot start camera'
            }), 500
            
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/camera/stop', methods=['POST'])
def stop_camera():
    """D·ª´ng camera"""
    global camera_manager
    
    try:
        camera_manager.stop()
        return jsonify({
            'status': 'success',
            'message': 'Camera stopped'
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/camera/list', methods=['GET'])
def list_cameras():
    """Li·ªát k√™ c√°c camera c√≥ s·∫µn"""
    cameras = []
    
    for i in range(10):  # Th·ª≠ 10 camera index
        cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)  # DSHOW cho Windows
        if cap.isOpened():
            cameras.append({
                'index': i,
                'name': f'Camera {i}',
                'resolution': f'{int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}',
                'fps': int(cap.get(cv2.CAP_PROP_FPS))
            })
            cap.release()
    
    return jsonify({
        'status': 'success',
        'cameras': cameras
    })
    
@app.route('/api/status', methods=['GET'])
def get_status():
    """API tr·∫£ v·ªÅ tr·∫°ng th√°i AI model"""
    global ai_running, system
    
    try:
        backend_connected = False
        if system and hasattr(system, 'backend_sender'):
            backend_connected = system.backend_sender.is_connected
        
        return jsonify({
            'status': 'running' if ai_running else 'stopped',
            'ai_system_initialized': system is not None,
            'backend_connected': backend_connected,
            'camera_source': 'webcam',
            'has_trained_model': hasattr(system, 'svm_model') and system.svm_model is not None,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/control', methods=['POST'])
def control_model():
    """API ƒëi·ªÅu khi·ªÉn model t·ª´ web"""
    global ai_running, ai_thread, system
    
    data = request.get_json()
    if not data:
        print("‚ö†Ô∏è No JSON data received in request")
        return jsonify({'error': 'No data provided'}), 400
    
    action = data.get('action')
    
    if not action:
        print("‚ö†Ô∏è No 'action' field in request data")
        return jsonify({'error': 'No action specified'}), 400
    
    print(f"üì° Received action: {action}")
    
    if action == 'start':
        with ai_status_lock:
            if ai_running:
                return jsonify({
                    'status': 'already_running',
                    'message': 'AI is already running'
                })
            
            # Kh·ªüi t·∫°o h·ªá th·ªëng n·∫øu ch∆∞a c√≥
            if system is None:
                gpu_available, device = setup_gpu()
                system = CompleteRecognitionSystem(device=device)
                if not system.initialize_system():
                    return jsonify({
                        'error': 'Failed to initialize AI system'
                    }), 500
                
                # Th·ª≠ load trained model
                system.load_trained_model()
            
            # Start AI thread
            ai_running = True
            ai_thread = threading.Thread(target=ai_processing_loop, daemon=True)
            ai_thread.start()
            
            return jsonify({
                'status': 'success',
                'message': 'AI model started successfully',
                'device': system.device,
                'timestamp': datetime.now().isoformat()
            })
        
    elif action == 'stop':
        with ai_status_lock:
            if not ai_running:
                return jsonify({
                    'status': 'already_stopped',
                    'message': 'AI is already stopped'
                })
            
            ai_running = False
            
            # Ch·ªù thread d·ª´ng
            if ai_thread:
                ai_thread.join(timeout=3)
            
            return jsonify({
                'status': 'success',
                'message': 'AI model stopped successfully',
                'timestamp': datetime.now().isoformat()
            })
    
    else:
        return jsonify({'error': 'Invalid action'}), 400

@app.route('/api/start_ai', methods=['POST'])
def start_ai():
    """API kh·ªüi ƒë·ªông AI"""
    global ai_running, ai_thread, system
    
    print("üì° /api/start_ai endpoint called")
    
    with ai_status_lock:
        if ai_running:
            return jsonify({
                'status': 'already_running',
                'message': 'AI is already running',
                'timestamp': datetime.now().isoformat()
            })
        
        # Kh·ªüi t·∫°o h·ªá th·ªëng n·∫øu ch∆∞a c√≥
        if system is None:
            gpu_available, device = setup_gpu()
            system = CompleteRecognitionSystem(device=device)
            if not system.initialize_system():
                return jsonify({
                    'error': 'Failed to initialize AI system',
                    'timestamp': datetime.now().isoformat()
                }), 500
            
            # Th·ª≠ load trained model
            system.load_trained_model()
        
        # Start AI thread
        ai_running = True
        ai_thread = threading.Thread(target=ai_processing_loop, daemon=True)
        ai_thread.start()
        
        return jsonify({
            'status': 'success',
            'message': 'AI model started successfully',
            'device': system.device,
            'timestamp': datetime.now().isoformat()
        })

@app.route('/api/stop_ai', methods=['POST'])
def stop_ai():
    """API d·ª´ng AI"""
    global ai_running, ai_thread
    
    print("üì° /api/stop_ai endpoint called")
    
    with ai_status_lock:
        if not ai_running:
            return jsonify({
                'status': 'already_stopped',
                'message': 'AI is already stopped',
                'timestamp': datetime.now().isoformat()
            })
        
        ai_running = False
        
        # Ch·ªù thread d·ª´ng
        if ai_thread:
            ai_thread.join(timeout=3)
        
        return jsonify({
            'status': 'success',
            'message': 'AI model stopped successfully',
            'timestamp': datetime.now().isoformat()
        })

@app.route('/api/latest_results', methods=['GET'])
def get_latest_results():
    """API l·∫•y k·∫øt qu·∫£ detection m·ªõi nh·∫•t"""
    global last_detection_results, last_detection_time, detection_lock
    
    # üî¥ FIX: S·ª≠ d·ª•ng lock khi ƒë·ªçc d·ªØ li·ªáu
    with detection_lock:
        current_results = last_detection_results.copy() if last_detection_results else []
        current_time = last_detection_time
    
    if not current_results:
        return jsonify({
            'status': 'no_data',
            'message': 'No detection results available',
            'timestamp': datetime.now().isoformat()
        })
    
    # üî¥ TH√äM: Debug log ƒë·ªÉ bi·∫øt API ƒëang tr·∫£ v·ªÅ g√¨
    print(f"üì° API /latest_results: returning {len(current_results)} detections")
    
    return jsonify({
        'status': 'success',
        'count': len(current_results),
        'results': current_results,
        'last_update': current_time.isoformat() if current_time else None,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/engagement', methods=['GET'])
def get_engagement_data():
    """API l·∫•y d·ªØ li·ªáu engagement c·ªßa l·ªõp h·ªçc"""
    global system
    
    if not system or not hasattr(system, 'engagement_calculator'):
        return jsonify({'error': 'System not initialized'}), 400
    
    report = system.get_class_engagement_report()
    
    if report:
        return jsonify({
            'status': 'success',
            'report': report,
            'timestamp': datetime.now().isoformat()
        })
    else:
        return jsonify({
            'status': 'no_data',
            'message': 'No engagement data available',
            'timestamp': datetime.now().isoformat()
        })

@app.route('/api/debug/ai_status', methods=['GET'])
def debug_ai_status():
    """Debug endpoint ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i AI system"""
    global ai_running, system, last_detection_results, last_detection_time, ai_thread
    
    with detection_lock:
        results_count = len(last_detection_results) if last_detection_results else 0
        last_time = last_detection_time.isoformat() if last_detection_time else "None"
    
    return jsonify({
        'ai_running': ai_running,
        'system_initialized': system is not None,
        'detection_results_count': results_count,
        'last_detection_time': last_time,
        'thread_active': ai_thread is not None and ai_thread.is_alive() if ai_thread else False,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'ai_running': ai_running,
        'ai_system_initialized': system is not None,
        'camera_source': 'webcam',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/config', methods=['GET'])
def get_config():
    """Get current configuration"""
    global system
    
    config = {
        'camera_source': 'webcam',
        'ai_system': 'CompleteRecognitionSystem',
        'has_face_detector': system and hasattr(system, 'face_analyzer') and system.face_analyzer is not None,
        'has_behavior_detector': system and hasattr(system, 'behavior_detector') and system.behavior_detector is not None,
        'has_svm_model': system and hasattr(system, 'svm_model') and system.svm_model is not None,
        'has_backend_connection': system and hasattr(system, 'backend_sender') and system.backend_sender.is_connected,
        'has_engagement_calculator': system and hasattr(system, 'engagement_calculator')  # NEW
    }
    
    if system:
        config.update({
            'device': system.device,
            'model_name': system.model_name
        })
    
    return jsonify(config)

# ==================== AI PROCESSING LOOP ====================
def process_and_send_engagement_data(system, student_data_list):
    """X·ª≠ l√Ω v√† g·ª≠i engagement data ƒë·∫øn backend"""
    if not system or not student_data_list:
        return
    
    try:
        # Chu·∫©n b·ªã batch data
        engagement_batch = []
        
        for student in student_data_list:
            engagement_item = {
                "student_id": f"AI_{student.get('id', hash(str(student)) % 10000):04d}",
                "student_name": student.get('name', 'Unknown Student'),
                "name": student.get('name', 'Unknown Student'),
                "engagement_score": student.get('engagement', 75.0),
                "concentration_level": student.get('concentration_level', 'medium'),
                "emotion": student.get('emotion', 'neutral'),
                "behavior": student.get('behavior', 'normal'),
                "emotion_confidence": student.get('emotion_confidence', 0.5),
                "date": datetime.now().strftime("%Y-%m-%d"),
                "session_id": f"session_{int(time.time())}",
                "recorded_by": "AI Recognition System",
                "class_name": "AI Classroom"
            }
            engagement_batch.append(engagement_item)
        
        # G·ª≠i qua backend sender
        if hasattr(system, 'backend_sender') and system.backend_sender.is_connected:
            # Option 1: G·ª≠i batch qua endpoint m·ªõi
            system.backend_sender.send_engagement_batch(engagement_batch)
        
        print(f"üì§ Sent engagement data for {len(engagement_batch)} students")
        
    except Exception as e:
        print(f"‚ùå Error processing engagement data: {e}")
        
def ai_processing_loop():
    """Thread x·ª≠ l√Ω AI - D√ôNG CAMERA MANAGER"""
    global ai_running, system, last_detection_results, last_detection_time
    global camera_manager, detection_lock
    
    print("ü§ñ Starting AI processing loop with shared camera...")
    
    # Kh·ªüi ƒë·ªông camera v·ªõi retry
    max_retries = 5
    retry_count = 0
    
    while retry_count < max_retries and ai_running:
        if camera_manager.start():
            print("‚úÖ Camera started successfully")
            break
        else:
            retry_count += 1
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông camera, th·ª≠ l·∫°i l·∫ßn {retry_count}/{max_retries}")
            time.sleep(2)
    
    if not camera_manager.is_running:
        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông camera sau nhi·ªÅu l·∫ßn th·ª≠")
        with ai_status_lock:
            ai_running = False
        return
    
    frame_count = 0
    fps_time = time.time()
    fps_counter = 0
    last_batch_sent = 0
    consecutive_errors = 0
    max_consecutive_errors = 10
    
    while ai_running:
        try:
            # üî¥ ƒê·ªåC FRAME TR·ª∞C TI·∫æP T·ª™ CAMERA MANAGER
            frame = camera_manager.read_frame()
            
            if frame is None:
                consecutive_errors += 1
                if consecutive_errors > max_consecutive_errors:
                    print("üîÑ Qu√° nhi·ªÅu l·ªói li√™n ti·∫øp, ƒëang kh·ªüi ƒë·ªông l·∫°i camera...")
                    camera_manager.stop()
                    time.sleep(1)
                    if camera_manager.start():
                        consecutive_errors = 0
                        time.sleep(0.5)
                    else:
                        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông l·∫°i camera, d·ª´ng AI loop")
                        break
                else:
                    time.sleep(0.1)
                continue
            
            # Reset error counter
            consecutive_errors = 0
            
            frame_count += 1
            fps_counter += 1
            
            # T√≠nh FPS
            current_time = time.time()
            if current_time - fps_time >= 2.0:
                fps = fps_counter / (current_time - fps_time)
                fps_counter = 0
                fps_time = current_time
                # C√≥ th·ªÉ b·∫≠t log FPS khi debug
                if frame_count % 60 == 0:
                    print(f"üìä AI Loop FPS: {fps:.1f}, Frame: {frame_count}")
            
            # Process AI
            student_data_list = []
            if system and frame_count % 2 == 0:
                face_results = system.detect_faces(frame)
                behavior_results = []
                
                if hasattr(system.behavior_detector, 'pose_model'):
                    behavior_results = system.behavior_detector.detect_behavior(frame)
                
                if face_results:
                    student_data_list = system.process_frame_with_engagement(
                        frame, face_results, behavior_results
                    )
                
                # L∆∞u k·∫øt qu·∫£
                with detection_lock:
                    if student_data_list:
                        last_detection_results = student_data_list.copy()
                        last_detection_time = datetime.now()
                    else:
                        last_detection_results = []
                        last_detection_time = datetime.now()
                
                # ==================== üî¥ TH√äM: G·ª¨I BATCH DATA ====================
                if student_data_list and system.backend_sender.is_connected:
                    # G·ª≠i batch m·ªói 30 frames (~1 gi√¢y n·∫øu 30fps)
                    if frame_count - last_batch_sent >= 30:
                        try:
                            # Chu·∫©n b·ªã detections cho batch
                            detections_for_batch = []
                            for student in student_data_list:
                                # Map student_data sang format c·ªßa send_detection_batch
                                detection_item = {
                                    "name": student.get('name', 'Unknown'),
                                    "id": student.get('id', 0),
                                    "emotion": student.get('emotion', 'neutral'),
                                    "emotion_confidence": student.get('emotion_confidence', 0.5),
                                    "behavior": student.get('behavior', 'normal'),
                                    "engagement": student.get('engagement', 50.0),
                                    "bbox": [
                                        student['bbox']['x'],
                                        student['bbox']['y'], 
                                        student['bbox']['width'],
                                        student['bbox']['height']
                                    ] if 'bbox' in student else None,
                                    "face_confidence": student.get('face_confidence', 0.5)
                                }
                                detections_for_batch.append(detection_item)
                            
                            # G·ªçi send_detection_batch
                            if detections_for_batch:
                                success = system.backend_sender.send_detection_batch(
                                    detections=detections_for_batch,
                                    fps=fps,
                                    frame_count=frame_count
                                )
                                
                                if success:
                                    last_batch_sent = frame_count
                                    # Log nh·∫π ƒë·ªÉ kh√¥ng spam console
                                    if frame_count % 90 == 0:  # M·ªói 3 batch (3 gi√¢y)
                                        print(f"üì¶ AI Loop: Sent batch with {len(detections_for_batch)} detections")
                                        
                        except Exception as batch_error:
                            print(f"‚ö†Ô∏è Error in batch sending: {batch_error}")
                            # Kh√¥ng crash thread v√¨ l·ªói batch
            
            # ==================== üî¥ TH√äM: G·ª¨I ATTENDANCE DATA ====================
            # G·ª≠i attendance cho c√°c student detected
            if student_data_list and hasattr(system, 'attendance_system'):
                for student_data in student_data_list:
                    name = student_data.get('name', 'Unknown')
                    if name != "Unknown" and student_data.get('face_confidence', 0) > 0.6:
                        # G·ª≠i attendance v·ªõi t·∫ßn su·∫•t th·∫•p h∆°n
                        if frame_count % 60 == 0:  # M·ªói 2 gi√¢y
                            system.attendance_system.mark_attendance(
                                name=name,
                                emotion=student_data.get('emotion', 'neutral'),
                                emotion_confidence=student_data.get('emotion_confidence', 0.5),
                                behavior=student_data.get('behavior', 'normal'),
                                engagement=student_data.get('engagement', 50.0),
                                concentration_level=student_data.get('concentration_level', 'medium'),
                                confidence=student_data.get('face_confidence', 0.5)
                            )
            
            # Gi·ªØ FPS ·ªïn ƒë·ªãnh
            time.sleep(0.001)  # R·∫•t nh·ªè, v√¨ camera ƒë√£ c√≥ FPS c·ªë ƒë·ªãnh
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error in AI loop: {e}")
            import traceback
            traceback.print_exc()
            consecutive_errors += 1
            time.sleep(0.1)
    
    print("‚úÖ AI processing stopped")
    camera_manager.stop()


def generate_mjpeg():
    """Generate MJPEG stream t·ª´ camera manager - CH·ªà HI·ªÇN TH·ªä CAMERA TH∆Ø·ªúNG"""
    global camera_manager
    
    while True:
        # üî¥ L·∫§Y FRAME TR·ª∞C TI·∫æP T·ª™ CAMERA MANAGER
        frame = camera_manager.get_latest_frame()
        
        if frame is not None:
            try:
                # üî¥ CH·ªà HI·ªÇN TH·ªä CAMERA TH∆Ø·ªúNG, KH√îNG C√ì AI OVERLAY
                # Kh√¥ng x·ª≠ l√Ω AI, kh√¥ng v·∫Ω bounding box, kh√¥ng overlay
                
                # üî¥ TH√äM: Resize ƒë·ªÉ gi·∫£m bandwidth n·∫øu c·∫ßn
                display_frame = cv2.resize(frame, (640, 480))
                
                # Encode frame th√†nh JPEG
                ret, jpeg = cv2.imencode('.jpg', display_frame, 
                                         [cv2.IMWRITE_JPEG_QUALITY, 80])  # Ch·∫•t l∆∞·ª£ng v·ª´a
                if ret:
                    frame_bytes = jpeg.tobytes()
                    
                    # T·∫°o MJPEG frame
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + 
                           frame_bytes + b'\r\n')
            
            except Exception as e:
                # Log l·ªói nh·∫π
                print(f"Stream encode error: {e}")
                # V·∫´n yield frame r·ªóng ƒë·ªÉ kh√¥ng break stream
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + 
                       b'\r\n')
        else:
            # N·∫øu kh√¥ng c√≥ frame, ƒë·ª£i m·ªôt ch√∫t
            time.sleep(0.1)
        
        time.sleep(0.033)  # ~30 FPS

# ==================== C√ÅC H√ÄM PH·ª§ TR·ª¢ ====================
def create_folder_structure():
    """T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c"""
    folders = [
        "database",
        "database/person1",
        "database/person2", 
        "database/person3",
        "test_images"
    ]
    
    for folder in folders:
        os.makedirs(folder, exist_ok=True)
        print(f"‚úÖ ƒê√£ t·∫°o: {folder}/")
    
    print("\nüìÅ C·∫•u tr√∫c th∆∞ m·ª•c ƒë√£ ƒë∆∞·ª£c t·∫°o!")

def train_model():
    """Train model t·ª´ database"""
    gpu_available, device = setup_gpu()
    system = CompleteRecognitionSystem(device=device)
    
    if not system.initialize_system():
        return
    
    if not os.path.exists("database"):
        os.makedirs("database")
        print("üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c 'database'")
        print("üí° H√£y th√™m ·∫£nh c·ªßa b·∫°n v√†o th∆∞ m·ª•c database/person1, database/person2, etc.")
        return
    
    success = system.train_face_recognition()
    if success:
        print("üéâ Train model th√†nh c√¥ng!")
    else:
        print("‚ùå Train model th·∫•t b·∫°i!")

def view_attendance():
    """Xem l·ªãch s·ª≠ ƒëi·ªÉm danh"""
    attendance_system = AttendanceSystem()
    attendance_system.view_attendance()

def test_backend_connection():
    """Ki·ªÉm tra k·∫øt n·ªëi backend"""
    sender = EnhancedBackendDataSender()
    if sender.is_connected:
        print("‚úÖ K·∫øt n·ªëi backend: TH√ÄNH C√îNG")
    else:
        print("‚ùå K·∫øt n·ªëi backend: TH·∫§T B·∫†I")

def troubleshoot_gpu():
    """Kh·∫Øc ph·ª•c s·ª± c·ªë GPU"""
    print("\n" + "="*60)
    print("üîß KH·∫ÆC PH·ª§C S·ª∞ C·ªê GPU")
    print("="*60)
    
    print("1. üìã Ki·ªÉm tra card ƒë·ªì h·ªça:")
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ NVIDIA GPU ƒë∆∞·ª£c ph√°t hi·ªán")
            print(result.stdout.split('\n')[0])  # Hi·ªÉn th·ªã d√≤ng ƒë·∫ßu ti√™n
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y NVIDIA GPU ho·∫∑c driver")
    except:
        print("‚ùå Kh√¥ng th·ªÉ ch·∫°y nvidia-smi")
    
    print("\n2. üîÑ C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support:")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n3. üéØ C√†i ƒë·∫∑t Ultralytics ·ªïn ƒë·ªãnh:")
    print("   pip install ultralytics==8.0.196")
    
    print("\n4. üîß Fix numpy version issue:")
    print("   pip install numpy==1.24.3")
    print("   Ho·∫∑c th√™m code fix:")
    print("   import numpy")
    print("   if hasattr(numpy, '_core'):")
    print("       numpy.core.multiarray = numpy._core.multiarray")
    
    print("="*60)

def start_flask_server():
    """Kh·ªüi ƒë·ªông Flask server"""
    print("\n" + "="*80)
    print("üåê FLASK API SERVER")
    print("="*80)
    print("üì° Endpoints:")
    print("   ‚Ä¢ GET  /api/status         - Check AI status")
    print("   ‚Ä¢ POST /api/control        - Control AI (action: start/stop)")
    print("   ‚Ä¢ POST /api/start_ai       - Start AI model")
    print("   ‚Ä¢ POST /api/stop_ai        - Stop AI model")
    print("   ‚Ä¢ GET  /api/latest_results - Get latest detection results")
    print("   ‚Ä¢ GET  /api/engagement     - Get engagement report (NEW)")
    print("   ‚Ä¢ GET  /api/health         - Health check")
    print("   ‚Ä¢ GET  /api/config         - Get configuration")
    print("   ‚Ä¢ GET  /api/debug/ai_status - Debug AI status")
    print("="*80)
    print("üéØ AI System: Ready to be controlled via API")
    print("üìä Engagement System: Calculates focus score based on emotion and behavior")
    print("üì∑ Camera source: Webcam Direct (Camera 0)")  # üî¥ S·ª¨A D√íNG N√ÄY
    print("üìä Backend connection: Will send attendance, emotion, behavior, engagement data")
    print("="*80)
    print("üöÄ Starting Flask server on http://localhost:5000")
    
    # Ch·∫°y Flask server
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)

# ==================== REAL-TIME RECOGNITION ====================
def real_time_recognition():
    """Ch·∫°y real-time recognition v·ªõi camera d√πng chung cho AI v√† streaming"""
    global camera_manager, system
    
    # Ki·ªÉm tra v√† thi·∫øt l·∫≠p GPU
    gpu_available, device = setup_gpu()
    
    # Kh·ªüi t·∫°o h·ªá th·ªëng
    system = CompleteRecognitionSystem(device=device)
    
    if not system.initialize_system():
        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o h·ªá th·ªëng")
        return
    
    model_loaded = system.load_trained_model()
    if not model_loaded:
        print("‚ö†Ô∏è Ch·∫°y ·ªü ch·∫ø ƒë·ªô ch·ªâ detect c·∫£m x√∫c v√† h√†nh vi")
    
    # ==================== TH√äM: Camera Manager (D√ôNG CHUNG) ====================
    camera_manager = CameraManager(camera_index=0)
    
    # Th·ª≠ kh·ªüi ƒë·ªông camera v·ªõi retry
    max_retries = 3
    for attempt in range(max_retries):
        print(f"üîç ƒêang kh·ªüi ƒë·ªông camera (l·∫ßn th·ª≠ {attempt + 1}/{max_retries})...")
        if camera_manager.start():
            break
        else:
            if attempt < max_retries - 1:
                print(f"‚ö†Ô∏è Kh√¥ng th√†nh c√¥ng, th·ª≠ l·∫°i sau 2 gi√¢y...")
                time.sleep(2)
            else:
                print("‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông camera sau nhi·ªÅu l·∫ßn th·ª≠!")
                
                # H·ªèi ng∆∞·ªùi d√πng c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng (ch·ªâ d√πng streaming)
                choice = input("üö´ Kh√¥ng th·ªÉ k·∫øt n·ªëi camera. B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c ch·ªâ v·ªõi streaming? (y/n): ")
                if choice.lower() != 'y':
                    return
                else:
                    print("‚ö†Ô∏è Ch·∫°y ·ªü ch·∫ø ƒë·ªô kh√¥ng c√≥ camera - ch·ªâ streaming")
    
    if camera_manager.is_running:
        print(f"\nüé• Camera {camera_manager.camera_index} ƒë√£ kh·ªüi ƒë·ªông")
    else:
        print(f"\n‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng c√≥ camera, ch·ªâ hi·ªÉn th·ªã streaming")
    
    print("üìä Ch·∫ø ƒë·ªô: AI Recognition + Live Streaming")
    print("üåê Stream URL: http://localhost:5000/video_feed")
    print("üéÆ Nh·∫•n 'q' ƒë·ªÉ tho√°t, 's' ƒë·ªÉ ch·ª•p ·∫£nh, 'v' ƒë·ªÉ xem ƒëi·ªÉm danh, 'e' ƒë·ªÉ xem engagement report")
    
    # ==================== TH√äM: Improved Face Tracker ====================
    class SimpleFaceTracker:
        def __init__(self, max_disappeared=15):
            self.next_id = 0
            self.objects = {}  # id -> {'bbox': ..., 'last_seen': frame_count}
            self.max_disappeared = max_disappeared
            self.frame_count = 0
        
        def update(self, detected_bboxes):
            """Update tracking v·ªõi detected bboxes"""
            self.frame_count += 1
            
            # Gi·∫£m last_seen cho t·∫•t c·∫£ objects hi·ªán c√≥
            for obj_id in list(self.objects.keys()):
                self.objects[obj_id]['last_seen'] += 1
                
                # X√≥a object n·∫øu kh√¥ng th·∫•y qu√° l√¢u
                if self.objects[obj_id]['last_seen'] > self.max_disappeared:
                    del self.objects[obj_id]
            
            # N·∫øu kh√¥ng c√≥ detection, return empty list
            if not detected_bboxes:
                return []
            
            # G√°n ID cho c√°c bbox m·ªõi
            assigned_ids = []
            
            for bbox in detected_bboxes:
                x, y, w, h = bbox
                center_x = x + w/2
                center_y = y + h/2
                
                # T√¨m object g·∫ßn nh·∫•t
                min_distance = float('inf')
                matched_id = None
                
                for obj_id, obj_data in self.objects.items():
                    obj_bbox = obj_data['bbox']
                    obj_center_x = obj_bbox[0] + obj_bbox[2]/2
                    obj_center_y = obj_bbox[1] + obj_bbox[3]/2
                    
                    distance = np.sqrt((center_x - obj_center_x)**2 + (center_y - obj_center_y)**2)
                    
                    # N·∫øu kho·∫£ng c√°ch < 100 pixels v√† object ch∆∞a ƒë∆∞·ª£c g√°n
                    if distance < 100 and distance < min_distance and obj_id not in assigned_ids:
                        min_distance = distance
                        matched_id = obj_id
                
                if matched_id is not None:
                    # C·∫≠p nh·∫≠t existing object
                    self.objects[matched_id]['bbox'] = bbox
                    self.objects[matched_id]['last_seen'] = 0
                    assigned_ids.append(matched_id)
                else:
                    # T·∫°o object m·ªõi
                    new_id = self.next_id
                    self.objects[new_id] = {
                        'bbox': bbox,
                        'last_seen': 0,
                        'created_at': self.frame_count
                    }
                    assigned_ids.append(new_id)
                    self.next_id += 1
            
            return assigned_ids  # Tr·∫£ v·ªÅ list ID theo th·ª© t·ª± detected_bboxes
    
    # ==================== TH√äM: Display Stabilizer ====================
    class DisplayStabilizer:
        def __init__(self):
            self.face_display_data = {}
            self.min_display_time = 0.5  # ‚ö° GI·∫¢M t·ª´ 2.0s xu·ªëng 0.5s
            self.last_update_time = {}
            self.smoothing_factor = 0.3  # Th√™m smoothing
            
        def get_stable_display(self, face_key, new_data, current_time):
            """L·∫•y d·ªØ li·ªáu hi·ªÉn th·ªã SMOOTH"""
            if face_key not in self.face_display_data:
                # Kh·ªüi t·∫°o m·ªõi
                self.face_display_data[face_key] = {
                    'behavior': new_data.get('behavior', 'normal'),
                    'name': new_data.get('name', 'Unknown'),
                    'emotion': new_data.get('emotion', 'neutral'),
                    'engagement': new_data.get('engagement', 50.0),
                    'bbox_smooth': new_data.get('bbox', {}),  # Th√™m bbox smoothing
                    'last_update': current_time
                }
                return self.face_display_data[face_key]
            
            # √Åp d·ª•ng smoothing cho bbox
            old_bbox = self.face_display_data[face_key].get('bbox_smooth', {})
            new_bbox = new_data.get('bbox', {})
            
            if old_bbox and new_bbox:
                smoothed_bbox = {
                    'x': int(old_bbox.get('x', 0) * 0.7 + new_bbox.get('x', 0) * 0.3),
                    'y': int(old_bbox.get('y', 0) * 0.7 + new_bbox.get('y', 0) * 0.3),
                    'width': int(old_bbox.get('width', 0) * 0.7 + new_bbox.get('width', 0) * 0.3),
                    'height': int(old_bbox.get('height', 0) * 0.7 + new_bbox.get('height', 0) * 0.3)
                }
                self.face_display_data[face_key]['bbox_smooth'] = smoothed_bbox
            
            # Update c√°c th√¥ng tin kh√°c v·ªõi smoothing
            update_threshold = 0.8  # Ch·ªâ update n·∫øu confidence cao
            
            # Behavior: ch·ªâ update n·∫øu behavior m·ªõi c√≥ confidence cao
            new_behavior = new_data.get('behavior', 'normal')
            old_behavior = self.face_display_data[face_key]['behavior']
            
            if new_behavior != old_behavior:
                behavior_conf = new_data.get('behavior_confidence', 0.5)
                if behavior_conf > update_threshold:
                    self.face_display_data[face_key]['behavior'] = new_behavior
                # Ho·∫∑c √°p d·ª•ng gradual change
                elif random.random() < 0.3:  # 30% chance ƒë·ªÉ update
                    self.face_display_data[face_key]['behavior'] = new_behavior
            
            # C·∫≠p nh·∫≠t timestamp
            self.face_display_data[face_key]['last_update'] = current_time
            
            return self.face_display_data[face_key]
    
    # Kh·ªüi t·∫°o trackers v√† stabilizers
    face_tracker = SimpleFaceTracker(max_disappeared=20)
    display_stabilizer = DisplayStabilizer()
    
    attendance_status = {}
    frame_count = 0
    
    # Bi·∫øn ƒë·ªÉ ƒëo FPS
    fps_counter = 0
    fps_time = time.time()
    
    # Bi·∫øn ƒë·ªÉ tracking face IDs qua c√°c frames
    tracked_face_ids = {}
    
    # ==================== FIX: Th√™m h√†m _match_face_to_behavior_improved ====================
    def _match_face_to_behavior_improved(face_data, behavior_results, face_bboxes_list, face_ids_list):
        """Improved version c·ªßa _match_face_to_behavior v·ªõi tracking"""
        face_bbox = face_data['bbox']
        x, y, w, h = face_bbox
        
        # T√¨m tracking ID cho face n√†y (n·∫øu c√≥)
        face_id = None
        for idx, bbox in enumerate(face_bboxes_list):
            bx, by, bw, bh = bbox
            # Ki·ªÉm tra overlap
            intersection_x1 = max(x, bx)
            intersection_y1 = max(y, by)
            intersection_x2 = min(x + w, bx + bw)
            intersection_y2 = min(y + h, by + bh)
            
            if intersection_x2 > intersection_x1 and intersection_y2 > intersection_y1:
                if idx < len(face_ids_list):
                    face_id = face_ids_list[idx]
                    break
        
        if not behavior_results:
            return {'type': 'normal', 'confidence': 0.7}
        
        best_match = {'type': 'normal', 'confidence': 0.7, 'distance': float('inf')}
        
        for behavior in behavior_results:
            if behavior['bbox'] is not None:
                try:
                    bx1, by1, bx2, by2 = behavior['bbox'].astype(int)
                    # T√≠nh trung ƒëi·ªÉm c·ªßa bbox
                    face_center_x = x + w/2
                    face_center_y = y + h/2
                    behavior_center_x = (bx1 + bx2) / 2
                    behavior_center_y = (by1 + by2) / 2
                    
                    # T√≠nh kho·∫£ng c√°ch Euclid
                    distance = np.sqrt((face_center_x - behavior_center_x)**2 + (face_center_y - behavior_center_y)**2)
                    
                    # T√≠nh IoU (Intersection over Union)
                    intersection_x1 = max(x, bx1)
                    intersection_y1 = max(y, by1)
                    intersection_x2 = min(x + w, bx2)
                    intersection_y2 = min(y + h, by2)
                    
                    if intersection_x2 > intersection_x1 and intersection_y2 > intersection_y1:
                        intersection_area = (intersection_x2 - intersection_x1) * (intersection_y2 - intersection_y1)
                        face_area = w * h
                        behavior_area = (bx2 - bx1) * (by2 - by1)
                        union_area = face_area + behavior_area - intersection_area
                        
                        iou = intersection_area / union_area if union_area > 0 else 0
                        
                        # Gi·∫£m distance n·∫øu c√≥ overlap t·ªët
                        if iou > 0.3:
                            distance *= 0.3
                        elif iou > 0.1:
                            distance *= 0.7
                    
                    if distance < best_match['distance']:
                        best_match = {
                            'type': behavior['behavior'],
                            'confidence': min(0.9, max(0.7, 1 - distance/300)),
                            'distance': distance,
                            'iou': iou if 'iou' in locals() else 0
                        }
                except Exception as e:
                    continue
        
        return best_match
    
    # M·ªü c·ª≠a s·ªï preview
    cv2.namedWindow('AI Face Recognition + Streaming Preview', cv2.WINDOW_NORMAL)
    
    while True:
        try:
            # üî¥ ƒê·ªåC FRAME TR·ª∞C TI·∫æP T·ª™ CAMERA MANAGER (D√ôNG CHUNG)
            frame = camera_manager.read_frame()
            
            if frame is None:
                print("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame t·ª´ camera")
                time.sleep(0.1)
                continue
            
            frame_count += 1
            fps_counter += 1
            
            # T√≠nh FPS
            current_time = time.time()
            if current_time - fps_time >= 1.0:
                fps = fps_counter / (current_time - fps_time)
                fps_counter = 0
                fps_time = current_time
                fps_text = f"FPS: {fps:.1f}"
            else:
                fps_text = "FPS: calculating..."
            
            # Gi·∫£m t·∫ßn su·∫•t detection ƒë·ªÉ tƒÉng performance
            detection_interval = 3
            student_data_list = []
            face_results = []
            behavior_results = []
            
            if frame_count % detection_interval == 0:
                # Ph√°t hi·ªán khu√¥n m·∫∑t
                face_results = system.detect_faces(frame)
                
                # Ph√°t hi·ªán h√†nh vi m·ªói 6 frames
                if frame_count % 6 == 0 and hasattr(system.behavior_detector, 'pose_model'):
                    behavior_results = system.behavior_detector.detect_behavior(frame)
                
                # ==================== TH√äM: Face Tracking ====================
                # T·∫°o bboxes t·ª´ face_results ƒë·ªÉ tracking
                face_bboxes = []
                for face in face_results:
                    x, y, w, h = face['bbox']
                    face_bboxes.append([x, y, w, h])
                
                # Update face tracking
                face_ids = []
                if face_bboxes:
                    face_ids = face_tracker.update(face_bboxes)
                    # G√°n ID cho faces
                    for idx, face in enumerate(face_results):
                        if idx < len(face_ids):
                            face['tracking_id'] = face_ids[idx]
                            tracked_face_ids[face_ids[idx]] = current_time
                
                # ==================== X·ª¨ L√ù AI V√Ä ENGAGEMENT ====================
                for i, face_data in enumerate(face_results):
                    bbox = face_data['bbox']
                    x, y, w, h = bbox
                    emotion = face_data['emotion']
                    emotion_conf = face_data['emotion_confidence']
                    
                    if hasattr(system, 'svm_model') and system.svm_model:
                        name, confidence = system.recognize_face(face_data)
                    else:
                        name, confidence = "Unknown", 0.0
                    
                    # S·ª≠ d·ª•ng h√†m matching improved v·ªõi tracking
                    matched_behavior = _match_face_to_behavior_improved(
                        face_data, 
                        behavior_results,
                        face_bboxes,
                        face_ids
                    )
                    
                    behavior = matched_behavior['type']
                    behavior_confidence = matched_behavior['confidence']
                    
                    # T√≠nh engagement score
                    engagement_result = system.engagement_calculator.calculate_engagement(
                        student_id=f"{name}_{i}",
                        emotion=emotion,
                        emotion_confidence=emotion_conf,
                        behavior=behavior,
                        behavior_confidence=behavior_confidence,
                        bbox=(x, y, w, h)
                    )
                    
                    student_data = {
                        'id': i + 1,
                        'name': name,
                        'emotion': emotion,
                        'emotion_confidence': emotion_conf,
                        'behavior': behavior,
                        'engagement': engagement_result['engagement_score'],
                        'concentration_level': engagement_result['concentration_level'],
                        'bbox': {'x': int(x), 'y': int(y), 'width': int(w), 'height': int(h)},
                        'face_confidence': confidence,
                        'tracking_id': face_ids[i] if i < len(face_ids) else i,
                        'engagement_details': engagement_result
                    }
                    
                    student_data_list.append(student_data)
                
                # ==================== G·ª¨I D·ªÆ LI·ªÜU ƒê·∫æN BACKEND ====================
                for student_data in student_data_list:
                    name = student_data['name']
                    emotion = student_data['emotion']
                    emotion_conf = student_data['emotion_confidence']
                    behavior = student_data['behavior']
                    engagement = student_data['engagement']
                    concentration_level = student_data['concentration_level']
                    confidence = student_data['face_confidence']
                    
                    # G·ª≠i d·ªØ li·ªáu ƒëi·ªÉm danh, c·∫£m x√∫c, h√†nh vi, ƒë·ªô t·∫≠p trung
                    if name != "Unknown" and confidence > 0.6:
                        # T·∫°o unique key v·ªõi tracking_id
                        tracking_id = student_data.get('tracking_id', hash(str(student_data['bbox'])) % 10000)
                        attendance_key = f"{name}_{tracking_id}"
                        
                        if attendance_key not in attendance_status or frame_count % 30 == 0:
                            system.attendance_system.mark_attendance(
                                name=name,
                                emotion=emotion,
                                emotion_confidence=emotion_conf,
                                behavior=behavior,
                                engagement=engagement,
                                concentration_level=concentration_level,
                                confidence=confidence
                            )
                            attendance_status[attendance_key] = True
                
                # üî¥ C·∫¨P NH·∫¨T K·∫æT QU·∫¢ CHO STREAMING
                with detection_lock:
                    if student_data_list:
                        last_detection_results = student_data_list.copy()
                        last_detection_time = datetime.now()
            
            # ==================== V·∫º OVERLAY AI L√äN FRAME ====================
            overlay_frame = frame.copy()
            
            # V·∫Ω overlay cho m·ªói face
            for i, student_data in enumerate(student_data_list):
                bbox = student_data['bbox']
                x, y, w, h = bbox['x'], bbox['y'], bbox['width'], bbox['height']
                
                # T·∫°o face_key unique
                tracking_id = student_data.get('tracking_id', i)
                face_key = f"face_{tracking_id}"
                
                # L·∫•y d·ªØ li·ªáu hi·ªÉn th·ªã ƒë√£ ƒë∆∞·ª£c stabilized
                display_data = display_stabilizer.get_stable_display(
                    face_key, 
                    student_data, 
                    current_time
                )
                
                # L·∫•y th√¥ng tin t·ª´ display_data ƒë√£ stabilized
                name = display_data['name']
                emotion = display_data['emotion']
                behavior = display_data['behavior']
                engagement = display_data['engagement']
                
                # L·∫•y confidence v√† concentration_level t·ª´ student_data g·ªëc
                confidence = student_data.get('face_confidence', 0.5)
                concentration_level = student_data.get('concentration_level', 'medium')
                emotion_conf = student_data.get('emotion_confidence', 0.5)
                
                # M√†u s·∫Øc
                color = (0, 255, 0) if name != "Unknown" else (0, 0, 255)
                
                # V·∫Ω bounding box
                cv2.rectangle(overlay_frame, (x, y), (x + w, y + h), color, 2)
                
                # ==================== HI·ªÇN TH·ªä TH√îNG TIN ====================
                # D√≤ng 1: T√™n v√† confidence
                info_text = f"{name} ({confidence:.2f})"
                cv2.putText(overlay_frame, info_text, (x, y - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                # D√≤ng 2: H√†nh vi (ƒê√É ƒê∆Ø·ª¢C STABILIZED)
                behavior_display = f"{behavior}"
                # M√†u cho behavior
                behavior_color = (255, 255, 0)  # V√†ng m·∫∑c ƒë·ªãnh
                
                if 'raising' in behavior:
                    behavior_color = (0, 255, 255)  # V√†ng ƒë·∫≠m cho gi∆° tay
                elif 'writing' in behavior:
                    behavior_color = (255, 255, 0)  # V√†ng s√°ng cho vi·∫øt
                elif 'look_around' in behavior:
                    behavior_color = (0, 165, 255)  # Cam cho nh√¨n quanh
                
                cv2.putText(overlay_frame, behavior_display, (x, y - 35), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, behavior_color, 1)
                
                # D√≤ng 3: C·∫£m x√∫c
                emotion_text = f"{emotion} ({emotion_conf:.1f})"
                emotion_color = (0, 255, 255)  # V√†ng cho c·∫£m x√∫c
                cv2.putText(overlay_frame, emotion_text, (x, y + h + 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, emotion_color, 1)
                
                # D√≤ng 4: Engagement score
                engagement_text = f"Engagement: {engagement:.0f} ({concentration_level})"
                # M√†u theo engagement level
                engagement_color = system._get_engagement_color(engagement)
                cv2.putText(overlay_frame, engagement_text, (x, y + h + 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, engagement_color, 1)
                
                # ==================== V·∫º ENGAGEMENT BAR ====================
                bar_width = 100
                bar_height = 8
                bar_x = x
                bar_y = y + h + 60
                
                # T√≠nh filled width d·ª±a tr√™n engagement (0-100)
                filled_width = int(bar_width * engagement / 100)
                
                # V·∫Ω thanh n·ªÅn
                cv2.rectangle(overlay_frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), 
                             (100, 100, 100), -1)
                
                # V·∫Ω thanh gi√° tr·ªã v·ªõi gradient color
                engagement_color = system._get_engagement_color(engagement)
                cv2.rectangle(overlay_frame, (bar_x, bar_y), (bar_x + filled_width, bar_y + bar_height), 
                             engagement_color, -1)
                
                # V·∫Ω vi·ªÅn cho thanh
                cv2.rectangle(overlay_frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), 
                             (200, 200, 200), 1)
                
                # Hi·ªÉn th·ªã tracking ID (nh·ªè, ƒë·ªÉ debug)
                tracking_text = f"ID: {tracking_id}"
                cv2.putText(overlay_frame, tracking_text, (x + w - 40, y + 15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
            
            # ==================== HI·ªÇN TH·ªä STATUS BAR ====================
            backend_status = "üü¢ REAL-TIME" if system.backend_sender.is_connected else "üî¥ OFFLINE"
            device_status = "‚ö° GPU" if gpu_available else "üíª CPU"
            
            # Status bar ch√≠nh
            info_text = f"Camera {camera_manager.camera_index} | Faces: {len(face_results)} | Backend: {backend_status} | Device: {device_status} | {fps_text}"
            cv2.putText(overlay_frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # D√≤ng th√¥ng tin ph·ª•
            try:
                # T√≠nh engagement trung b√¨nh
                if student_data_list and len(student_data_list) > 0:
                    avg_engagement = np.mean([s.get('engagement', 50) for s in student_data_list])
                    total_students = len(student_data_list)
                    
                    # ƒê·∫øm s·ªë h·ªçc sinh t·∫≠p trung
                    engaged_count = sum(1 for s in student_data_list if s.get('engagement', 0) >= 70)
                    distracted_count = sum(1 for s in student_data_list if s.get('engagement', 0) < 50)
                    
                    engagement_summary = f"Students: {total_students} | Avg Eng: {avg_engagement:.1f} | Focused: {engaged_count} | Distracted: {distracted_count}"
                    cv2.putText(overlay_frame, engagement_summary, (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    # Hi·ªÉn th·ªã tracking info
                    tracking_info = f"Tracked Faces: {len(tracked_face_ids)} | Frame: {frame_count}"
                    cv2.putText(overlay_frame, tracking_info, (10, 90), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            except Exception as e:
                # B·ªè qua l·ªói n·∫øu c√≥ v·∫•n ƒë·ªÅ v·ªõi d·ªØ li·ªáu
                pass
            
            # ==================== HI·ªÇN TH·ªä PREVIEW ====================
            cv2.imshow('AI Face Recognition + Streaming Preview', overlay_frame)
            
            # ==================== X·ª¨ L√ù PH√çM ====================
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("\nüõë Stopping recognition and streaming...")
                break
            elif key == ord('s'):
                # L∆∞u ·∫£nh ch·ª•p m√†n h√¨nh
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"capture_{timestamp}.jpg"
                cv2.imwrite(filename, overlay_frame)
                print(f"‚úÖ ƒê√£ l∆∞u ·∫£nh: {filename}")
                
                # L∆∞u th√™m file txt v·ªõi th√¥ng tin detection
                info_filename = f"capture_{timestamp}_info.txt"
                with open(info_filename, 'w') as f:
                    f.write(f"Capture time: {datetime.now().isoformat()}\n")
                    f.write(f"FPS: {fps if 'fps' in locals() else 0}\n")
                    f.write(f"Faces detected: {len(student_data_list)}\n")
                    f.write(f"Frame count: {frame_count}\n")
                    f.write("\nDetected faces:\n")
                    for i, student in enumerate(student_data_list):
                        f.write(f"\nFace {i+1}:\n")
                        f.write(f"  Name: {student.get('name', 'Unknown')}\n")
                        f.write(f"  Emotion: {student.get('emotion', 'neutral')}\n")
                        f.write(f"  Behavior: {student.get('behavior', 'normal')}\n")
                        f.write(f"  Engagement: {student.get('engagement', 0):.1f}\n")
                        f.write(f"  Confidence: {student.get('face_confidence', 0):.2f}\n")
                print(f"‚úÖ ƒê√£ l∆∞u th√¥ng tin: {info_filename}")
                
            elif key == ord('v'):
                # Xem attendance
                print("\n" + "="*80)
                print("üìã ATTENDANCE RECORDS")
                print("="*80)
                system.attendance_system.view_attendance()
                print("="*80)
                
            elif key == ord('e'):
                # Xem engagement report
                report = system.get_class_engagement_report()
                if report:
                    print("\n" + "="*80)
                    print("üìä ENGAGEMENT REPORT")
                    print("="*80)
                    print(f"Total Students: {report['total_students']}")
                    print(f"Average Engagement: {report['average_engagement']}")
                    print(f"Concentration Distribution:")
                    for level, count in report['concentration_distribution'].items():
                        percentage = (count / report['total_students'] * 100) if report['total_students'] > 0 else 0
                        print(f"  {level}: {count} students ({percentage:.1f}%)")
                    
                    print("\nTop 5 Students:")
                    sorted_students = sorted(report['students'], 
                                            key=lambda x: x['engagement'], 
                                            reverse=True)[:5]
                    for i, student in enumerate(sorted_students):
                        print(f"{i+1}. {student['name']}: {student['engagement']} ({student['concentration_level']})")
                    print("="*80)
                else:
                    print("üì≠ No engagement data available")
                    
            elif key == ord('d'):
                # Debug info
                print("\n" + "="*80)
                print("üêõ DEBUG INFORMATION")
                print("="*80)
                print(f"Frame count: {frame_count}")
                print(f"Current FPS: {fps if 'fps' in locals() else 'calculating...'}")
                print(f"Face results: {len(face_results)}")
                print(f"Behavior results: {len(behavior_results)}")
                print(f"Student data: {len(student_data_list)}")
                print(f"Tracked faces: {len(tracked_face_ids)}")
                print(f"Display stabilizer: {len(display_stabilizer.face_display_data)} entries")
                
                # Hi·ªÉn th·ªã tracking info
                if tracked_face_ids:
                    print(f"\nTracked IDs (last {min(10, len(tracked_face_ids))}):")
                    sorted_ids = sorted(tracked_face_ids.items(), 
                                       key=lambda x: x[1], 
                                       reverse=True)[:10]
                    for face_id, last_seen in sorted_ids:
                        age = current_time - last_seen
                        print(f"  ID {face_id}: last seen {age:.1f}s ago")
                print("="*80)
                
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ö†Ô∏è Error in main loop: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(0.1)
    
    # Cleanup
    camera_manager.stop()
    cv2.destroyAllWindows()
    
    # Print summary
    print("\n" + "="*80)
    print("üìä RECOGNITION SESSION SUMMARY")
    print("="*80)
    print(f"Total frames processed: {frame_count}")
    print(f"Device used: {'GPU' if gpu_available else 'CPU'}")
    print(f"Backend connection: {'Connected' if system.backend_sender.is_connected else 'Disconnected'}")
    
    if hasattr(system, 'attendance_system'):
        try:
            df = pd.read_csv(system.attendance_system.csv_file)
            print(f"Attendance records: {len(df)} entries")
        except:
            print(f"Attendance records: Unknown")
    
    print("üëã Session ended!")
    print("="*80)
# ==================== MAIN MENU ====================
def main_menu():
    """Hi·ªÉn th·ªã menu ch√≠nh"""
    # Ki·ªÉm tra h·ªá th·ªëng chi ti·∫øt
    check_system_capabilities()
    
    while True:
        print("\n" + "="*80)
        print("üé≠ COMPLETE RECOGNITION SYSTEM - FACE + EMOTION + BEHAVIOR + ENGAGEMENT + ATTENDANCE")
        print("="*80)
        print("1. üìÅ T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c")
        print("2. üéØ Train face recognition model")
        print("3. üé• Real-time (Face + Emotion + Behavior + Engagement + Attendance + Backend)")
        print("4. üìä Xem l·ªãch s·ª≠ ƒëi·ªÉm danh")
        print("5. üîó Ki·ªÉm tra k·∫øt n·ªëi backend")
        print("6. üîß Kh·∫Øc ph·ª•c s·ª± c·ªë GPU")
        print("7. üåê Start Flask API Server (for web control)")
        print("8. üö™ Tho√°t")
        print("="*80)
        print("üìä H·ªá th·ªëng t√≠nh engagement d·ª±a tr√™n c·∫£m x√∫c v√† h√†nh vi:")
        print("   - C·∫£m x√∫c: happy(0.85), neutral(0.7), sad(0.4), angry(0.3)")
        print("   - H√†nh vi: writing(0.9), look_straight(0.8), raising_hand(0.75)")
        print("   - K·∫øt qu·∫£: 0-100 ƒëi·ªÉm, 5 m·ª©c ƒë·ªô t·∫≠p trung")
        print("="*80)
        
        choice = input("üëâ Ch·ªçn ch·ª©c nƒÉng (1-8): ").strip()
        
        if choice == "1":
            create_folder_structure()
        elif choice == "2":
            train_model()
        elif choice == "3":
            real_time_recognition()
        elif choice == "4":
            view_attendance()
        elif choice == "5":
            test_backend_connection()
        elif choice == "6":
            troubleshoot_gpu()
        elif choice == "7":
            start_flask_server()
        elif choice == "8":
            print("üëã T·∫°m bi·ªát!")
            break
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
        
        if choice != "7":  # Kh√¥ng c·∫ßn nh·∫•n Enter n·∫øu ƒëang ch·∫°y Flask
            input("\nüëâ Nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")

# ==================== MAIN ====================
if __name__ == "__main__":
    print("üîß ƒêang ki·ªÉm tra h·ªá th·ªëng...")
    install_dependencies()
    
    print("\n" + "="*80)
    print("üéØ COMPLETE RECOGNITION SYSTEM WITH ENGAGEMENT SCORING")
    print("="*80)
    print("üìä T√≠nh nƒÉng:")
    print("   ‚Ä¢ Nh·∫≠n di·ªán khu√¥n m·∫∑t (InsightFace)")
    print("   ‚Ä¢ Nh·∫≠n di·ªán c·∫£m x√∫c (DeepFace)")
    print("   ‚Ä¢ Nh·∫≠n di·ªán h√†nh vi (YOLOv8-Pose)")
    print("   ‚Ä¢ T√≠nh ƒëi·ªÉm t·∫≠p trung (Engagement Score):")
    print("     üìà D·ª±a tr√™n c·∫£m x√∫c + h√†nh vi")
    print("     üéØ 0-100 ƒëi·ªÉm, 5 m·ª©c ƒë·ªô t·∫≠p trung")
    print("     ‚öñÔ∏è Tr·ªçng s·ªë khoa h·ªçc cho t·ª´ng y·∫øu t·ªë")
    print("   ‚Ä¢ ƒêi·ªÉm danh t·ª± ƒë·ªông")
    print("   ‚Ä¢ Backend integration - G·ª≠i to√†n b·ªô d·ªØ li·ªáu:")
    print("     üìã ƒêi·ªÉm danh (attendance)")
    print("     üòä C·∫£m x√∫c (emotion)")
    print("     üéØ H√†nh vi (behavior)")
    print("     üìä ƒê·ªô t·∫≠p trung (engagement)")
    print("   ‚Ä¢ üåê Flask API Server (port 5000)")
    print("="*80)
    print("üöÄ Ch·∫°y option 7 ƒë·ªÉ kh·ªüi ƒë·ªông Flask API Server")
    print("üåê Web frontend c√≥ th·ªÉ g·ªçi API t·∫°i: http://localhost:5000")
    print("üìä Engagement API: /api/engagement - L·∫•y b√°o c√°o t·∫≠p trung l·ªõp h·ªçc")
    print("üìä Backend: http://localhost:8000")
    print("="*80)
    
    main_menu()